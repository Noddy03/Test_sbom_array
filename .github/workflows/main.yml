name: Fetch Top IoT Repositories + Trivy/Conan SBOM Scan (NTIA + BSI V2)

# Trigger
on:
  workflow_dispatch:
    inputs:
      language:
        description: "Programming language to scan (C++, Python, Java, etc.)"
        required: true
        default: "C++"

# Minimal permissions required
permissions:
  contents: read
  security-events: write
  actions: read

jobs:
  # ================================================================
  # 1. FETCH TOP REPOS  (UNCHANGED â€” DO NOT TOUCH)
  # ================================================================
  fetch_top_repos:
    runs-on: ubuntu-latest
    outputs:
      repo_list: ${{ steps.fetch.outputs.repo_list }}

    steps:
      - name: Install jq
        run: |
          sudo apt-get update -y
          sudo apt-get install -y jq

      - name: Fetch repositories  # <--- DO NOT TOUCH (AS REQUESTED)
        id: fetch
        env:
          LANG: ${{ github.event.inputs.language }}
          GH_TOKEN: ${{ github.token }}
        run: |
          set -euo pipefail
          RAW_Q="topic:IoT language:$LANG fork:false archived:false"
          echo "Query: $RAW_Q"
          REPOS=$(gh api -X GET /search/repositories \
             --raw-field q="$RAW_Q" \
             --raw-field sort=stars \
             --raw-field order=desc \
             --raw-field per_page=50 \
             --jq '.items[].full_name'
          )
          echo "Found repos:"
          echo "$REPOS"
          JSON_ARRAY=$(printf "%s\n" "$REPOS" | jq -R -s -c 'split("\n")[:-1]')
          echo "repo_list=$JSON_ARRAY" >> $GITHUB_OUTPUT

  # ================================================================
  # 2. SBOM SCAN + ENRICHMENT (YAML A - full)
  # ================================================================
  sbom_scan:
    needs: fetch_top_repos
    runs-on: ubuntu-latest
    continue-on-error: true

    strategy:
      fail-fast: false
      matrix:
        repo: ${{ fromJSON(needs.fetch_top_repos.outputs.repo_list) }}

    steps:
      # ------------------------------------------------------------
      # Install required tools: conan, jq, trivy
      # ------------------------------------------------------------
      - name: Install Conan + jq + Trivy
        run: |
          set -euo pipefail
          sudo apt-get update -y
          sudo apt-get install -y jq wget curl python3-pip
          pip install --upgrade pip
          pip install conan
          # Install trivy (adds trivy executable to /usr/local/bin)
          curl -sfL https://raw.githubusercontent.com/aquasecurity/trivy/main/contrib/install.sh \
            | sudo sh -s -- -b /usr/local/bin

      # ------------------------------------------------------------
      # Checkout repository (shallow clone) - skip if cannot clone
      # ------------------------------------------------------------
      - name: Checkout repository
        env:
          REPO: ${{ matrix.repo }}
          TOKEN: ${{ secrets.MY_PERSONAL_TOKEN || github.token }}
        run: |
          set -euo pipefail
          if git clone --depth=1 "https://$TOKEN@github.com/$REPO" project-src; then
              echo "skip_repo=false" >> $GITHUB_ENV
          else
              echo "skip_repo=true" >> $GITHUB_ENV
          fi

      # --------------------------------------------------------------------
      # 2A â€” GITHUB LICENSE API (best-effort)
      # Use Accept header and fallback to NOASSERTION
      # --------------------------------------------------------------------
      - name: Fetch GitHub license metadata
        if: env.skip_repo != 'true'
        env:
          REPO: ${{ matrix.repo }}
          GH_TOKEN: ${{ github.token }}
        run: |
          set -euo pipefail
          # Preferred Accept header; fall back to NOASSERTION on error/404
          gh api -H "Accept: application/vnd.github+json" "/repos/$REPO/license" > project-src/gh-license.json 2>/dev/null \
            || echo '{"license":{"spdx_id":"NOASSERTION"}}' > project-src/gh-license.json

      # --------------------------------------------------------------------
      # 2B â€” LOCAL LICENSE DETECTION (fallback)
      # Detect common license filenames (LICENSE, COPYING, etc.)
      # --------------------------------------------------------------------
      - name: Local license file detection (fallback)
        if: env.skip_repo != 'true'
        run: |
          set -euo pipefail
          echo "ðŸ”Ž Searching for local license files..."
          FILE=$(find project-src -maxdepth 4 -type f \
            -iregex ".*\(LICENSE\|LICENCE\|COPYING\|COPYRIGHT\|LICENSE\.md\|LICENSE\.txt\)" \
            | head -1 || true)
          if [ -n "$FILE" ]; then
            echo "ðŸ“„ Found local license file: $FILE"
            cp "$FILE" project-src/local-license.txt
          else
            echo "âš ï¸ No local license file found."
            echo "" > project-src/local-license.txt
          fi

      # --------------------------------------------------------------------
      # 2C â€” BUILD SBOMs (Conan + Trivy)
      # Conan for dependency info (if present), Trivy for CycloneDX SBOM
      # --------------------------------------------------------------------
      - name: Conan SBOM
        if: env.skip_repo != 'true'
        run: |
          set -euo pipefail
          cd project-src || exit 1
          conan profile detect --force || true
          conan graph info . --format=json > conan-sbom.json || echo '{"components":[]}' > conan-sbom.json

      - name: Trivy SBOM (CycloneDX)
        if: env.skip_repo != 'true'
        run: |
          set -euo pipefail
          # Generate CycloneDX SBOM from filesystem
          trivy fs --scanners vuln,license --format cyclonedx \
            -o project-src/trivy-fs.json project-src \
            || echo '{"components":[]}' > project-src/trivy-fs.json

      # --------------------------------------------------------------------
      # 2D â€” MERGE LICENSE SIGNALS (GitHub -> Trivy -> Local -> NOASSERTION)
      # Guarantees metadata.licenses and per-component licenses
      # --------------------------------------------------------------------
      - name: Merge GitHub + Trivy + Local license into SBOM
        if: env.skip_repo != 'true'
        run: |
          set -euo pipefail
          SBOM="project-src/trivy-fs.json"
          GHLIC="project-src/gh-license.json"
          LOCALLIC="project-src/local-license.txt"

          # Extract SPDX from GitHub / Trivy
          GH_SPDX=$(jq -r '.license.spdx_id // empty' "$GHLIC" 2>/dev/null || echo "")
          TRIVY_SPDX=$(jq -r '
            [.components[]?.licenses[]?.license?.id // empty] |
            map(select(. != "")) | unique | join(",")
          ' "$SBOM" 2>/dev/null || echo "")

          LOCAL_SPDX=""
          if [ -s "$LOCALLIC" ]; then LOCAL_SPDX="CUSTOM"; fi

          if [ -n "$GH_SPDX" ] && [ "$GH_SPDX" != "NOASSERTION" ]; then
            FINAL="$GH_SPDX"
          elif [ -n "$TRIVY_SPDX" ]; then
            FINAL="$TRIVY_SPDX"
          elif [ -n "$LOCAL_SPDX" ]; then
            FINAL="$LOCAL_SPDX"
          else
            FINAL="NOASSERTION"
          fi

          # Ensure SBOM exists
          if [ ! -f "$SBOM" ] || [ ! -s "$SBOM" ]; then
            echo '{"bomFormat":"CycloneDX","components":[],"metadata":{}}' > "$SBOM"
          fi

          # Inject merged license into metadata and components where missing
          jq --arg lic "$FINAL" '
            .metadata.licenses = [{ "license": { "id": $lic }}] |
            .components |= map(
              if (.licenses | length) == 0 then .licenses = [{ "license": { "id": $lic }}] else . end
            )
          ' "$SBOM" > tmp.json && mv tmp.json "$SBOM"

      # --------------------------------------------------------------------
      # 2E â€” ENRICH SBOM FOR NTIA + BSI V2 (component-level, robust jq)
      # Adds: component.supplier, component.hashes (SBOM-level), externalReferences, metadata.bomLinks,
      # metadata.tools/properties, signature, timestamp.
      # --------------------------------------------------------------------
      - name: Enrich SBOM for NTIA + BSI v2 (component-level)
        if: env.skip_repo != 'true'
        env:
          REPO: ${{ matrix.repo }}
        run: |
          set -euo pipefail
          SBOM="project-src/trivy-fs.json"
          REPO_URL="https://github.com/$REPO"
          OWNER=$(echo "$REPO" | cut -d'/' -f1)

          # Ensure SBOM file exists and is valid JSON
          if [ ! -f "$SBOM" ] || [ ! -s "$SBOM" ]; then
            echo '{"bomFormat":"CycloneDX","components":[],"metadata":{}}' > "$SBOM"
          fi

          # Compute SBOM-level SHA256 signature (fast Option 1)
          FULL_SHA=$(sha256sum "$SBOM" | awk '{print $1}')
          echo "SBOM SHA256: $FULL_SHA"

          # Robust jq transformation ensuring no object+array add errors:
          # normalize metadata arrays then add the required fields and per-component entries
          jq --arg owner "$OWNER" --arg repo "$REPO" --arg repourl "$REPO_URL" --arg sha "$FULL_SHA" '
            # Normalize metadata arrays (if objects -> wrap in array; if missing -> empty array)
            .metadata.tools |= (if type=="object" then [.] elif type=="array" then . else [] end) |
            .metadata.properties |= (if type=="object" then [.] elif type=="array" then . else [] end) |
            .metadata.bomLinks |= (if type=="object" then [.] elif type=="array" then . else [] end) |
            .metadata.externalReferences |= (if type=="object" then [.] elif type=="array" then . else [] end) |

            # Add/merge tool info and build metadata
            .metadata.tools += [{
              "vendor": "Aqua Security",
              "name": "Trivy",
              "version": "CycloneDX"
            }] |
            .metadata.properties += [
              {"name":"build.environment","value":"GitHub Actions (ubuntu-latest)"},
              {"name":"build.timestamp","value":(now|todate)}
            ] |

            # Supplier metadata at SBOM level (still add per-component below)
            .metadata.supplier = {
              "name": $owner,
              "url": ("https://github.com/" + $owner)
            } |

            # Add or append bomLinks
            .metadata.bomLinks += [
              {"rel":"self","href":("https://github.com/" + $repo + "/sbom")},
              {"rel":"repository","href": $repourl},
              {"rel":"distribution","href": ($repourl + "/releases")}
            ] |

            # Signature block (simple SBOM-level signature using computed sha)
            .signature = {"algorithm":"SHA-256","value":$sha} |

            # Ensure components exists as array
            .components |= (if . == null then [] else . end) |

            # Per-component enrichment: supplier, hashes, externalReferences, ensure license exists
            .components |= map(
              (. // {}) |
              .supplier = (.supplier // {"name": $owner, "url": ("https://github.com/" + $owner)}) |
              .hashes = (if (.hashes // []) | length == 0 then [{"alg":"SHA-256","content":$sha}] else .hashes end) |
              .externalReferences = ((.externalReferences // []) + [
                {"type":"vcs","url": $repourl},
                {"type":"distribution","url": ($repourl + "/releases")}
              ]) |
              .licenses = (if (.licenses // []) | length == 0 then [{ "license": { "id": "NOASSERTION" }}] else .licenses end)
            )
          ' "$SBOM" > tmp.json && mv tmp.json "$SBOM"

          echo "âœ” Enrichment complete: component.supplier, component.hashes, externalReferences, metadata, bomLinks, signature inserted."

      # --------------------------------------------------------------------
      # 2F â€” SBOMQS COMPLIANCE ANALYSIS (NTIA + BSI) and Score with creation_info injection
      # We make sure these steps always produce JSON (fallback to {}), then inject creation_info into score.json
      # --------------------------------------------------------------------
      - name: NTIA Compliance
        if: env.skip_repo != 'true'
        run: |
          set -euo pipefail
          # Run compliance NTIA; if it fails, write empty JSON so aggregator can still run
          docker run --rm -v $PWD/project-src:/sbom ghcr.io/interlynk-io/sbomqs:latest \
            compliance --ntia /sbom/trivy-fs.json \
            > project-src/compliance_NTIA.json || echo '{}' > project-src/compliance_NTIA.json

      - name: BSI v2 Compliance
        if: env.skip_repo != 'true'
        run: |
          set -euo pipefail
          docker run --rm -v $PWD/project-src:/sbom ghcr.io/interlynk-io/sbomqs:latest \
            compliance --bsi-v2 /sbom/trivy-fs.json \
            > project-src/compliance_BSI.json || echo '{}' > project-src/compliance_BSI.json

      - name: Score SBOM (with creation_info injection)
        if: env.skip_repo != 'true'
        env:
          REPO: ${{ matrix.repo }}
        run: |
          set -euo pipefail
          # Run score; fallback to empty JSON if the container errors
          docker run --rm -v $PWD/project-src:/sbom ghcr.io/interlynk-io/sbomqs:latest \
            score /sbom/trivy-fs.json --json \
            > project-src/score.json || echo '{}' > project-src/score.json

          # Inject creation_info block so aggregator can extract the expected columns.
          # creation_info fields: name, version, scoring_engine_version, vendor
          # We try to preserve any existing scoring_engine_version in the score output.
          jq --arg repo "$REPO" '
            .creation_info = {
              "name": "SBOMQS Score",
              "version": "1.0",
              "scoring_engine_version": (.scoring_engine_version // "unknown"),
              "vendor": $repo
            }
          ' project-src/score.json > project-src/score.tmp.json && mv project-src/score.tmp.json project-src/score.json

      # --------------------------------------------------------------------
      # 2G â€” Packaging / Upload artifacts (unchanged behaviour)
      # --------------------------------------------------------------------
      - name: Sanitize artifact name
        id: sanitize
        run: |
          set -euo pipefail
          SAFE="${{ matrix.repo }}"
          SAFE="${SAFE//\//_}"
          echo "name=$SAFE" >> $GITHUB_OUTPUT

      - name: Upload SBOM reports
        uses: actions/upload-artifact@v4
        with:
          name: sbom-reports-${{ steps.sanitize.outputs.name }}
          path: project-src/*.json
          if-no-files-found: warn

  # ================================================================
  # 3. AGGREGATION SUMMARY â€” FIX MODE A (produce compact summary CSV)
  # ================================================================
  aggregate_results:
    name: Aggregate SBOMQS Results
    needs: sbom_scan
    runs-on: ubuntu-latest

    steps:
      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: results/

      - name: Install Python + Pandas
        run: |
          sudo apt-get update && sudo apt-get install -y python3-pip
          pip install pandas

      - name: Aggregate into sbomqs_summary.csv (match example layout)
        run: |
          python3 <<'EOF'
          import json, glob, os, pandas as pd, time

          def load_json_safe(p):
              try:
                  with open(p) as f:
                      return json.load(f)
              except Exception:
                  return None

          rows = []
          run_id = os.getenv("GITHUB_RUN_ID", "")
          timestamp = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())

          # Find all score.json files under results/
          score_paths = glob.glob("results/**/score.json", recursive=True)

          # For each score.json, build the output row matching your sample
          for sp in sorted(score_paths):
              # artifact directory (folder that contains this score.json)
              artifact_dir = os.path.dirname(sp)
              repo = os.path.basename(artifact_dir)

              score = load_json_safe(sp)
              if not isinstance(score, dict):
                  continue

              # Prefer "files" array in score.json; fallback to trying other keys
              files_arr = score.get("files") or []
              files_list = []

              for f in files_arr:
                  # pick the fields similar to your example row
                  entry = {
                      "file_name": f.get("file_name") or f.get("path") or f.get("source", ""),
                      "spec": f.get("spec", ""),
                      "spec_version": f.get("spec_version") or f.get("specVersion") or "",
                      "file_format": f.get("file_format") or f.get("fileFormat") or "",
                      "avg_score": f.get("avg_score") or f.get("avg_score", None),
                      "num_components": f.get("num_components") or f.get("num_components", None),
                      "creation_time": f.get("creation_time") or f.get("creation_time", "") or f.get("creationTime", ""),
                      "gen_tool_name": f.get("gen_tool_name") or f.get("genToolName") or f.get("gen_tool", ""),
                      "gen_tool_version": f.get("gen_tool_version") or f.get("genToolVersion") or f.get("gen_tool_version", ""),
                      "scores": f.get("scores", []),
                  }
                  files_list.append(entry)

              # If files_list empty, attempt to build minimal file entry from score root (best-effort)
              if not files_list:
                  # try to extract some top-level info
                  entry = {
                      "file_name": "trivy-fs.json",
                      "spec": score.get("spec", ""),
                      "spec_version": score.get("spec_version", ""),
                      "file_format": score.get("file_format", ""),
                      "avg_score": score.get("avg_score", None),
                      "num_components": score.get("num_components", None),
                      "creation_time": score.get("timestamp", ""),
                      "gen_tool_name": score.get("gen_tool_name", ""),
                      "gen_tool_version": score.get("gen_tool_version", ""),
                      "scores": score.get("scores", []),
                  }
                  files_list.append(entry)

              # creation_info fields: derive from first file's scoring_engine where possible
              creation_name = ""
              creation_version = ""
              creation_engine_version = ""
              creation_vendor = ""

              if files_list:
                  first = files_list[0]
                  # score JSON sometimes nests scoring_engine inside the file record
                  scoring_engine = None
                  # attempt to find scoring_engine in original score file structure
                  # check score['files'][0]['scoring_engine'] if present
                  try:
                      orig_first = files_arr[0] if files_arr else {}
                      scoring_engine = orig_first.get("scoring_engine") or orig_first.get("scoringEngine") or None
                  except Exception:
                      scoring_engine = None

                  if scoring_engine and isinstance(scoring_engine, dict):
                      creation_name = scoring_engine.get("name", "") or creation_name
                      creation_version = scoring_engine.get("version", "") or creation_version
                      creation_vendor = scoring_engine.get("vendor", "") or creation_vendor

                  # fallbacks
                  if not creation_name:
                      creation_name = score.get("creation_info", {}).get("name") or score.get("tool", "") or "SBOMQS Score"
                  if not creation_version:
                      creation_version = score.get("creation_info", {}).get("version") or "1.0"
                  # gen tool version: try file.gen_tool_version, else score-level field
                  creation_engine_version = first.get("gen_tool_version") or score.get("gen_tool_version") or score.get("scoring_engine_version") or ""

              # Build output row
              row = {
                  "run_id": run_id,
                  "timestamp": timestamp,
                  "files": json.dumps(files_list, ensure_ascii=False),
                  "repository": repo,
                  "creation_info.name": creation_name,
                  "creation_info.version": creation_version,
                  "creation_info.scoring_engine_version": creation_engine_version,
                  "creation_info.vendor": creation_vendor
              }
              rows.append(row)

          # write CSV if any rows found
          if rows:
              df = pd.DataFrame(rows, columns=[
                  "run_id",
                  "timestamp",
                  "files",
                  "repository",
                  "creation_info.name",
                  "creation_info.version",
                  "creation_info.scoring_engine_version",
                  "creation_info.vendor"
              ])
              df.to_csv("sbomqs_summary.csv", index=False)
              print("Generated sbomqs_summary.csv with", len(rows), "rows")
          else:
              print("No score.json files found; sbomqs_summary.csv not created")
          EOF

      - name: Upload aggregated summary
        uses: actions/upload-artifact@v4
        with:
          name: sbomqs-summary
          path: sbomqs_summary.csv
