name: IoT Full SBOM Pipeline (Model 2 - Cdxgen-first, Build-before-scan, CycloneDX 1.5)

on:
  workflow_dispatch:
    inputs:
      language:
        description: "Language filter (e.g. C++, Python, Java)"
        required: true
        default: "C++"

permissions:
  contents: read
  security-events: write
  actions: read

env:
  DEBIAN_FRONTEND: noninteractive
  CYCLONEDX_SPEC: "1.5"

jobs:
  fetch_top_repos:
    runs-on: ubuntu-latest
    outputs:
      repo_list: ${{ steps.fetch.outputs.repo_list }}
    steps:
      - name: Install jq & gh
        run: |
          sudo apt-get update -y
          sudo apt-get install -y jq gh
      - name: Fetch repositories
        id: fetch
        env:
          LANG: ${{ github.event.inputs.language }}
          GH_TOKEN: ${{ github.token }}
        run: |
          set -euo pipefail
          RAW_Q="topic:iot language:$LANG fork:false archived:false"
          REPOS=$(gh api -X GET /search/repositories \
              --raw-field q="$RAW_Q" \
              --raw-field sort=stars \
              --raw-field order=desc \
              --raw-field per_page=100 \
              --jq '.items[].full_name' || true)
          if [ -z "$REPOS" ]; then
            RAW_Q="language:$LANG fork:false archived:false"
            REPOS=$(gh api -X GET /search/repositories \
              --raw-field q="$RAW_Q" \
              --raw-field sort=stars \
              --raw-field order=desc \
              --raw-field per_page=100 \
              --jq '.items[].full_name' || true)
          fi
          JSON_ARRAY=$(printf "%s\n" "$REPOS" | jq -R -s -c 'split("\n")[:-1]')
          echo "repo_list=$JSON_ARRAY" >> $GITHUB_OUTPUT
  sbom_scan:
    needs: fetch_top_repos
    continue-on-error: true
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        repo: ${{ fromJSON(needs.fetch_top_repos.outputs.repo_list) }}

    steps:
      - name: Install base deps
        run: |
          sudo apt-get update -y
          sudo apt-get install -y jq curl wget python3-pip gnupg lsb-release rpm nodejs npm git ca-certificates build-essential cmake pkg-config
          pip3 install --upgrade pip
          pip3 install cyclonedx-bom cyclonedx-python-lib pandas || true
      - name: Ensure Docker usable (best-effort)
        run: |
          sudo groupadd docker || true
          sudo usermod -aG docker $USER || true
          sudo systemctl restart docker || true
          sudo docker --version || true
      - name: Install Syft (local)
        run: |
          mkdir -p $HOME/bin
          curl -sSfL https://raw.githubusercontent.com/anchore/syft/main/install.sh | sh -s -- -b $HOME/bin || true
          echo "$HOME/bin" >> $GITHUB_PATH || true
      - name: Install Trivy (deb)
        run: |
          set -euo pipefail
          curl -fsSL https://aquasecurity.github.io/trivy-repo/deb/public.key \
            | sudo gpg --dearmor -o /usr/share/keyrings/trivy.gpg || true
          echo "deb [signed-by=/usr/share/keyrings/trivy.gpg] https://aquasecurity.github.io/trivy-repo/deb $(lsb_release -cs) main" \
            | sudo tee /etc/apt/sources.list.d/trivy.list || true
          sudo apt-get update -y || true
          sudo apt-get install -y trivy || true
          sudo trivy --download-db-only || true
      - name: Checkout target repository (reliable clone)
        run: |
          set -euo pipefail
          rm -rf project-src
          git clone --depth 1 https://github.com/${{ matrix.repo }}.git project-src || (echo "git clone failed" >&2; exit 1)
          if [ ! -d project-src ] || [ -z "$(ls -A project-src)" ]; then
            echo "ERROR: project-src is empty after clone" >&2
            exit 1
          fi
          ls -la project-src | sed -n '1,200p'
      - name: GitHub license metadata
        env:
          REPO: ${{ matrix.repo }}
          GH_TOKEN: ${{ github.token }}
        run: |
          gh api "/repos/$REPO/license" > project-src/gh-license.json 2>/dev/null || echo '{"license":{"spdx_id":"NOASSERTION"}}' > project-src/gh-license.json || true
      - name: Local SPDX detection
        run: |
          FILE=$(find project-src -maxdepth=6 -type f -iregex ".*(LICENSE|COPYING|COPYRIGHT|LICENSE.md|LICENSE.txt)" | head -1 || true)
          if [ -n "$FILE" ]; then cp "$FILE" project-src/local-license.txt; else echo "" > project-src/local-license.txt; fi
      - name: Determine final repository license
        run: |
          GH=$(jq -r '.license.spdx_id // empty' project-src/gh-license.json)
          FILE=$(grep -Eo "SPDX-License-Identifier:\s*[A-Za-z0-9.\-+]+" project-src/local-license.txt | head -1 | awk -F': ' '{print $2}' || true)
          FINAL="${GH:-$FILE}"
          [ -z "$FINAL" ] && FINAL="NOASSERTION"
          echo "$FINAL" > project-src/final_license.txt
      - name: Best-effort package manager prep (npm/pip/conan)
        run: |
          set -euo pipefail
          if [ -f project-src/package-lock.json ] || [ -f project-src/pnpm-lock.yaml ] || [ -f project-src/yarn.lock ]; then
            (cd project-src && if command -v npm >/dev/null 2>&1; then npm ci --silent || npm install --silent || true; fi) || true
          fi
          if [ -f project-src/requirements.txt ]; then
            python3 -m pip install --upgrade pip || true
            python3 -m pip install -r project-src/requirements.txt --no-deps --target project-src/.sbom_deps || true
          fi
          if ls project-src/conanfile.* >/dev/null 2>&1 || [ -f project-src/conanfile.txt ] 2>/dev/null; then
            (cd project-src && conan profile detect --force || true; conan install . --build=missing) || true
          fi
      - name: Build project (C/C++ best-effort)
        run: |
          set -euo pipefail
          cd project-src || exit 1
          if [ -f CMakeLists.txt ]; then
            mkdir -p build && cd build
            cmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_EXPORT_COMPILE_COMMANDS=ON .. || true
            cmake --build . -- -j$(nproc) || true
            if [ -f compile_commands.json ]; then
              cp compile_commands.json .. || true
            fi
            cd ..
          else
            echo "No CMakeLists.txt, skipping build" >&2
          fi
      - name: Cdxgen SBOM (C/C++ enhanced, prefer npx, docker fallback)
        run: |
          set -euo pipefail
          # prefer npx (no global install)
          if command -v npx >/dev/null 2>&1; then
            (cd project-src && npx --yes @cyclonedx/cdxgen \
              -t cpp --deep --recurse --no-binaries --resolve-licence \
              --spec-version $CYCLONEDX_SPEC -o cdxgen-sbom.json .) || true
          fi
          if command -v cdxgen >/dev/null 2>&1; then
            (cd project-src && cdxgen -t cpp --deep --recurse --no-binaries --resolve-licence --spec-version $CYCLONEDX_SPEC -o cdxgen-sbom.json .) || true
          fi
          if [ ! -f project-src/cdxgen-sbom.json ] || [ ! -s project-src/cdxgen-sbom.json ]; then
            sudo docker run --rm -v "$PWD/project-src":/src ghcr.io/cyclonedx/cdxgen:latest \
              cdxgen -t cpp --deep --recurse --no-binaries --resolve-licence --spec-version $CYCLONEDX_SPEC -o /src/cdxgen-sbom.json /src || true
          fi
          # ensure minimal CycloneDX structure if not produced
          if [ ! -s project-src/cdxgen-sbom.json ]; then
            cat > project-src/cdxgen-sbom.json <<'JSON'
          {"bomFormat":"CycloneDX","specVersion":"1.5","version":1,"components":[]}
          JSON
          fi
      - name: Trivy FS SBOM (CycloneDX) + filesystem vulnerability scan
        run: |
          set -euo pipefail
          trivy fs --format cyclonedx -o project-src/trivy-fs.json project-src || echo '{"components":[]}' > project-src/trivy-fs.json
          trivy fs --format json --vuln-type library,os --security-checks vuln -o project-src/vulnerabilities_fs.json project-src || echo '{"Results":[]}' > project-src/vulnerabilities_fs.json
      - name: Syft FS SBOM (optional enrichment)
        run: |
          set -euo pipefail
          export PATH="$HOME/bin:$PATH"
          syft packages dir:project-src -o cyclonedx-json > project-src/syft-sbom.json || echo '{"components":[]}' > project-src/syft-sbom.json
      - name: Normalize SBOM arrays
        run: |
          for f in project-src/trivy-fs.json project-src/syft-sbom.json project-src/cdxgen-sbom.json project-src/conan-sbom.json; do
            if [ -f "$f" ]; then
              jq 'if has("components") then . else (. + {components: []}) end | .components |= (if type=="array" then . else [] end)' "$f" > tmp && mv tmp "$f"
            else
              echo '{"components":[]}' > "$f"
            fi
          done
      - name: Merge SBOMs correctly (NTIA+BSI merged)
        run: |
          set -euo pipefail
          echo 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 | base64 --decode > project-src/merged_and_enrich.py
          python3 project-src/merged_and_enrich.py

      - name: Enrich SBOM with file-level components (Option F)
        run: |
          set -euo pipefail
          cat > project-src/enrich_and_scan.py <<'PY'
          import os, json, hashlib, uuid, time, subprocess
          
          ROOT='project-src'
          SKIP_DIR_NAMES={'build','build_output','cmake-build-debug','cmake-build-release',
                          'sdk','toolchain','tools','espressif','esp-idf','components','.arduino','.cargo','.pio','.venv'}
          MAX_FILES=50000
          MAX_HASH_SIZE=5*1024*1024
          MAX_TOTAL_BYTES=1024*1024*1024
          MAX_DEPTH=10
          DIR_MAX_FILES=5000
          DIR_MAX_BYTES=200*1024*1024
          
          def safe_dir_size_and_count(path):
              total=0
              count=0
              try:
                  with os.scandir(path) as it:
                      for e in it:
                          try:
                              if e.is_file(follow_symlinks=False):
                                  count+=1
                                  total+=e.stat().st_size
                              elif e.is_dir(follow_symlinks=False):
                                  count+=1
                          except:
                              continue
              except:
                  return (0,0)
              return (count,total)
          
          def should_skip_dir(path):
              base=os.path.basename(path).lower()
              if base in SKIP_DIR_NAMES:
                  return True
              try:
                  cnt, sz = safe_dir_size_and_count(path)
                  if cnt > DIR_MAX_FILES or sz > DIR_MAX_BYTES:
                      return True
              except:
                  return True
              return False
          
          def is_binary(path):
              try:
                  with open(path,'rb') as f:
                      chunk = f.read(1024)
                      return b'\0' in chunk
              except:
                  return True
          
          def sha256_file(p):
              h=hashlib.sha256()
              with open(p,'rb') as f:
                  for chunk in iter(lambda:f.read(8192),b''):
                      h.update(chunk)
              return h.hexdigest()
          
          files=[]
          total_hash_bytes=0
          skipped_dirs_sample=[]
          
          # Walk repository safely
          for dirpath, dirnames, filenames in os.walk(ROOT, followlinks=False):
              rel=os.path.relpath(dirpath, ROOT)
              depth=0 if rel=='.' else rel.count(os.sep)+1
              if depth>MAX_DEPTH:
                  continue
              if should_skip_dir(dirpath):
                  skipped_dirs_sample.append(dirpath)
                  continue
              for fn in filenames:
                  full=os.path.join(dirpath, fn)
                  try:
                      size=os.path.getsize(full)
                  except:
                      continue
                  entry={'path':os.path.relpath(full, ROOT), 'size': size}
                  if size <= MAX_HASH_SIZE and total_hash_bytes + size <= MAX_TOTAL_BYTES and not is_binary(full):
                      try:
                          entry['sha256'] = sha256_file(full)
                          total_hash_bytes += size
                      except:
                          entry['sha256'] = None
                  else:
                      entry['sha256'] = None
                  files.append(entry)
                  if len(files) >= MAX_FILES:
                      break
              if len(files) >= MAX_FILES:
                  break
          
          # Load merged sbom and enrich
          try:
              data = json.load(open(os.path.join(ROOT,'merged-sbom.json')))
          except:
              data = {'components': []}
          
          try:
              sha = subprocess.check_output(['git','rev-parse','--short','HEAD'], cwd=ROOT).decode().strip()
          except:
              sha = '0'
          owner_repo = os.environ.get('GITHUB_REPOSITORY','')
          if not owner_repo:
              try:
                  remote = subprocess.check_output(['git','config','--get','remote.origin.url'], cwd=ROOT).decode().strip()
                  if remote.endswith('.git'):
                      remote = remote[:-4]
                  if remote.startswith('git@github.com:'):
                      owner_repo = remote.split(':',1)[1]
                  elif 'github.com/' in remote:
                      owner_repo = remote.split('github.com/',1)[1]
              except:
                  owner_repo = ''
          name = owner_repo.split('/')[-1] if owner_repo else os.path.basename(os.getcwd())
          version = sha
          
          existing = {(c.get('name'), c.get('version')) for c in data.get('components', [])}
          
          for f in files:
              nm = f['path']
              key = (nm, version)
              if key in existing:
                  continue
              comp = {
                  'type':'file',
                  'name': nm,
                  'version': version,
                  'purl': 'pkg:github/'+(owner_repo if owner_repo else name)+'@'+version,
                  'externalReferences': [{'type':'vcs','url':'https://github.com/'+(owner_repo if owner_repo else name)}],
                  'properties': [{'name':'size','value': str(f['size'])}]
              }
              if f.get('sha256'):
                  comp['hashes']=[{'alg':'SHA-256','content':f['sha256']}]
              if f['size'] > MAX_HASH_SIZE:
                  comp.setdefault('properties', []).append({'name':'large-file','value':'true'})
              data.setdefault('components', []).append(comp)
          
          data['serialNumber'] = data.get('serialNumber') or 'urn:uuid:'+str(uuid.uuid4())
          data['metadata'] = data.get('metadata') or {}
          data['metadata']['timestamp'] = int(time.time())
          data['metadata']['tools'] = data['metadata'].get('tools', [])
          data['metadata']['component'] = data['metadata'].get('component', {})
          data['metadata']['component']['name'] = name
          data['metadata']['component']['version'] = version
          data['repo_license'] = data.get('repo_license','NOASSERTION')
          
          json.dump(data, open(os.path.join(ROOT,'merged-sbom.json'), 'w'), indent=2)
          print('Option F enriched', len(files), 'files')
          if skipped_dirs_sample:
              print('skipped_dirs_sample:', skipped_dirs_sample[:10])
          
          # Safety guarded static analysis
          try:
              hashed_count = sum(1 for f in files if f.get('sha256'))
          except:
              hashed_count = 0
          
          try:
              if hashed_count <= 5000:
                  cmd = ['cppcheck','--enable=warning,style,performance,portability,information,missingInclude','--inline-suppr','--xml-version=2', ROOT]
                  out = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=False)
                  open(os.path.join(ROOT,'cppcheck_raw.xml'),'wb').write(out.stderr)
                  findings = []
                  try:
                      import xml.etree.ElementTree as ET
                      rootxml = ET.fromstring(out.stderr.decode(errors='ignore'))
                      for error in rootxml.findall('.//error'):
                          fn=error.get('file') or 'unknown'
                          msg=error.get('msg') or ''
                          severity=error.get('severity') or 'info'
                          findings.append({'id':'cppcheck-'+str(abs(hash(fn+msg)))[:12],'file':fn,'message':msg,'severity':severity})
                  except Exception:
                      pass
                  json.dump(findings, open(os.path.join(ROOT,'cppcheck.json'),'w'), indent=2)
              else:
                  print('Skipping cppcheck: hashed_count', hashed_count, 'too large')
                  json.dump([], open(os.path.join(ROOT,'cppcheck.json'),'w'), indent=2)
          except Exception as e:
              print('cppcheck failed', e)
              json.dump([], open(os.path.join(ROOT,'cppcheck.json'),'w'), indent=2)
          
          try:
              if len(files) <= 20000:
                  out = subprocess.run(['flawfinder','--quiet','--dataonly', ROOT], stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=False)
                  findings = []
                  for line in out.stdout.decode().splitlines():
                      parts=line.split(':',3)
                      if len(parts)>=4:
                          fn,ln,level,msg = parts[0],parts[1],parts[2],parts[3]
                          findings.append({'id':'flawfinder-'+str(abs(hash(fn+ln+msg)))[:12],'file':fn,'line':ln,'level':level,'message':msg})
                  json.dump(findings, open(os.path.join(ROOT,'flawfinder.json'),'w'), indent=2)
              else:
                  print('Skipping flawfinder: too many files', len(files))
                  json.dump([], open(os.path.join(ROOT,'flawfinder.json'),'w'), indent=2)
          except Exception as e:
              print('flawfinder failed', e)
              json.dump([], open(os.path.join(ROOT,'flawfinder.json'),'w'), indent=2)
          PY
          python3 project-src/enrich_and_scan.py || true
      - name: NTIA & BSI hybrid enrichment (fill missing metadata)


        run: |
          pip install cyclonedx-python-lib || true
          echo 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 | base64 --decode > project-src/enrich_ntia_bsi_hybrid.py
          python3 project-src/enrich_ntia_bsi_hybrid.py || true
    

      
      - name: Validate merged SBOM (universal CycloneDX 1.5 validator)
        run: |
          set -euo pipefail
          python3 <<'PY'
          import sys, json  
          try:
              with open("project-src/merged-sbom.json") as f:
                  data = json.load(f)
              # Minimal CycloneDX validity checks
              required_fields = ["bomFormat", "specVersion", "components"]
              missing = [f for f in required_fields if f not in data]
              if missing:
                  print("INVALID_SBOM: Missing fields:", missing, file=sys.stderr)
                  sys.exit(1)
              if not isinstance(data.get("components"), list):
                  print("INVALID_SBOM: components must be an array", file=sys.stderr)
                  sys.exit(1)
              # Very basic semantic checks
              if data["bomFormat"] not in ["CycloneDX"]:
                  print("INVALID_SBOM: wrong bomFormat", file=sys.stderr)
                  sys.exit(1)
              print("SBOM_VALID")
          except Exception as e:
              print("SBOM_VALIDATION_ERROR", e)
              sys.exit(1)
          PY
      - name: Trivy SBOM vulnerability scan (full check libraries + os)
        run: |
          set -euo pipefail
          trivy sbom --format json --vuln-type library,os -o project-src/vulnerabilities_sbom.json project-src/merged-sbom.json || echo '{"Results":[]}' > project-src/vulnerabilities_sbom.json
      - name: Create unified vulnerabilities list (json + csv)
        run: |
          python3 <<'PY'
          import json, csv
          def extract(path, source):
              try:
                  data=json.load(open(path))
              except:
                  return []
              vulns=[]
              for r in data.get("Results",[]):
                  for v in r.get("Vulnerabilities",[]) or []:
                      vulns.append({
                          "id": v.get("VulnerabilityID"),
                          "package": v.get("PkgName"),
                          "version": v.get("InstalledVersion"),
                          "severity": v.get("Severity"),
                          "cvss": v.get("CVSS",{}),
                          "description": (v.get("Description") or "").strip(),
                          "source": source
                      })
              return vulns
          fs = extract("project-src/vulnerabilities_fs.json","filesystem")
          sb = extract("project-src/vulnerabilities_sbom.json","sbom")
          allv = fs + sb
          json.dump(allv, open("project-src/vulnerabilities.json","w"), indent=2)
          with open("project-src/vulnerabilities.csv","w", newline='') as f:
              w = csv.writer(f)
              w.writerow(["id","package","version","severity","source","description"])
              for v in allv:
                  w.writerow([v.get("id"), v.get("package"), v.get("version"), v.get("severity"), v.get("source"), v.get("description")])
          PY
      - name: Create libraries list (json + csv)
        run: |
          python3 <<'PY'
          import json, csv
          try:
              data=json.load(open("project-src/merged-sbom.json"))
          except:
              data={"components":[]}
          libs=[]
          for c in data.get("components",[]):
              supplier = None
              supp = c.get("supplier")
              if isinstance(supp, dict):
                  supplier = supp.get("name")
              elif supp:
                  supplier = supp
              libs.append({
                  "name": c.get("name"),
                  "version": c.get("version"),
                  "purl": c.get("purl"),
                  "supplier": supplier
              })
          json.dump(libs, open("project-src/libraries.json","w"), indent=2)
          with open("project-src/libraries.csv","w", newline='') as f:
              w=csv.writer(f)
              w.writerow(["name","version","purl","supplier"])
              for l in libs:
                  w.writerow([l.get("name"), l.get("version"), l.get("purl"), l.get("supplier")])
          PY
      - name: NTIA compliance (json)
        run: |
          set -euo pipefail
          (sudo docker run --rm -v "$PWD"/project-src:/sbom ghcr.io/interlynk-io/sbomqs compliance --ntia --format json /sbom/merged-sbom.json > project-src/compliance_NTIA.json) || (echo '{"error":true}' > project-src/compliance_NTIA.json)
      - name: BSI-V2 compliance (json)
        run: |
          set -euo pipefail
          (sudo docker run --rm -v "$PWD"/project-src:/sbom ghcr.io/interlynk-io/sbomqs compliance --bsi-v2 --format json /sbom/merged-sbom.json > project-src/compliance_BSI.json) || (echo '{"error":true}' > project-src/compliance_BSI.json)
      - name: SBOM Score (json)
        run: |
          set -euo pipefail
          (sudo docker run --rm -v "$PWD"/project-src:/sbom ghcr.io/interlynk-io/sbomqs score /sbom/merged-sbom.json --json > project-src/score.json) || (echo '{"error":true}' > project-src/score.json)
      - name: Debug list project-src contents (before upload)
        run: |
          echo "---- project-src tree (max depth 3) ----"
          ls -la project-src | sed -n '1,200p'
          find project-src -maxdepth 3 -type f -print | sed -n '1,200p' || true
          echo "---- head of key files if present ----"
          for f in project-src/merged-sbom.json project-src/cdxgen-sbom.json project-src/trivy-fs.json project-src/vulnerabilities_fs.json project-src/vulnerabilities_sbom.json project-src/score.json project-src/summary.csv; do
              if [ -f "$f" ]; then
                  echo ">>> $f exists (size=$(stat -c%s "$f")), head:"
                  head -n 20 "$f" || true
              else
                  echo ">>> $f MISSING"
              fi
          done
      - name: Create per-repo summary.csv (before upload)
        run: |
          python3 <<'PY'
          import json, csv, os
          repo = "${{ matrix.repo }}"
          try:
              sbom = json.load(open("project-src/merged-sbom.json"))
              comp_count = len(sbom.get("components", []))
              repo_license = sbom.get("repo_license", "NOASSERTION")
          except:
              comp_count = 0
              repo_license = "NOASSERTION"
          try:
              vfs = json.load(open("project-src/vulnerabilities_fs.json"))
              vfs_count = sum(len(r.get("Vulnerabilities", []) or []) for r in vfs.get("Results", []))
          except:
              vfs_count = 0
          try:
              vsb = json.load(open("project-src/vulnerabilities_sbom.json"))
              vsb_count = sum(len(r.get("Vulnerabilities", []) or []) for r in vsb.get("Results", []))
          except:
              vsb_count = 0
          high = 0
          critical = 0
          for src in ("vulnerabilities_fs.json", "vulnerabilities_sbom.json"):
              try:
                  d = json.load(open(f"project-src/{src}"))
                  for r in d.get("Results", []):
                      for v in r.get("Vulnerabilities", []) or []:
                          sev = (v.get("Severity") or "").upper()
                          if sev == "HIGH": high += 1
                          if sev == "CRITICAL": critical += 1
              except:
                  pass
          try:
              score = json.load(open("project-src/score.json")).get("score", None)
          except:
              score = None
          def passcheck(fname):
              try:
                  d = json.load(open(fname))
                  return "error" not in d
              except:
                  return False
          ntia_pass = passcheck("project-src/compliance_NTIA.json")
          bsi_pass = passcheck("project-src/compliance_BSI.json")
          os.makedirs("project-src", exist_ok=True)
          with open("project-src/summary.csv", "w", newline='') as f:
              w = csv.writer(f)
              w.writerow([
                  "repo","repo_license","component_count","vuln_fs_total","vuln_sbom_total","vuln_total","vuln_high","vuln_critical","score_value","ntia_pass","bsi_pass"
              ])
              w.writerow([
                  repo,repo_license,comp_count,vfs_count,vsb_count,vfs_count+vsb_count,high,critical,score,str(ntia_pass),str(bsi_pass)
              ])
          PY
      - name: Ensure summary.csv exists (fallback)
        run: |
          if [ ! -f project-src/summary.csv ]; then
            echo "repo,repo_license,component_count,vuln_fs_total,vuln_sbom_total,vuln_total,vuln_high,vuln_critical,score_value,ntia_pass,bsi_pass" > project-src/summary.csv
            echo "${{ matrix.repo }},NOASSERTION,0,0,0,0,0,0,,false,false" >> project-src/summary.csv
          fi
      - name: Sanitize artifact name
        id: sanitize
        run: |
          SAFE="${{ matrix.repo }}"
          SAFE="${SAFE//\//_}"
          echo "name=$SAFE" >> $GITHUB_OUTPUT
      - name: Upload project-src as artifact (preserve structure)
        uses: actions/upload-artifact@v4
        with:
          name: ${{ steps.sanitize.outputs.name }}
          path: project-src/**
          if-no-files-found: warn

  aggregate_summaries:
    name: Aggregate all summary.csv files
    needs: sbom_scan
    runs-on: ubuntu-latest
    steps:
      - name: Download all repo artifacts
        uses: actions/download-artifact@v4
        with:
          path: all_summaries

      - name: Combine all summary.csv files into master_summary.csv
        run: |
          echo "repo,repo_license,component_count,vuln_fs_total,vuln_sbom_total,vuln_total,vuln_high,vuln_critical,score_value,ntia_pass,bsi_pass" > master_summary.csv
          found=0
          for f in $(find all_summaries -name "summary.csv"); do
            tail -n +2 "$f" >> master_summary.csv
            echo "Appended $f" >&2
            found=$((found+1))
          done
          echo "Appended $found per-repo summaries"
          if [ $found -eq 0 ]; then
            echo "WARNING: No per-repo summary.csv files found" >&2
          fi
      - name: Upload master_summary.csv
        uses: actions/upload-artifact@v4
        with:
          name: master_summary
          path: master_summary.csv

