name: IoT Repositories – Full SBOM (Trivy + Syft + Conan + Cdxgen) with NTIA + BSI-V2

on:
  workflow_dispatch:
    inputs:
      language:
        description: "Language filter"
        required: true
        default: "C++"

permissions:
  contents: read
  security-events: write
  actions: read

jobs:

  ##############################
  # FETCH TOP REPOS
  ##############################
  fetch_top_repos:
    runs-on: ubuntu-latest
    outputs:
      repo_list: ${{ steps.fetch.outputs.repo_list }}

    steps:
      - name: Install jq
        run: sudo apt-get update -y && sudo apt-get install -y jq

      - name: Fetch repositories
        id: fetch
        env:
          LANG: ${{ github.event.inputs.language }}
          GH_TOKEN: ${{ github.token }}
        run: |
          set -euo pipefail
          # NOTE: topic matching can be case-sensitive on GitHub; adjust if needed
          RAW_Q="topic:iot language:$LANG fork:false archived:false"
          REPOS=$(gh api -X GET /search/repositories \
              --raw-field q="$RAW_Q" \
              --raw-field sort=stars \
              --raw-field order=desc \
              --raw-field per_page=100 \
              --jq '.items[].full_name')
          JSON_ARRAY=$(printf "%s\n" "$REPOS" | jq -R -s -c 'split("\n")[:-1]')
          echo "repo_list=$JSON_ARRAY" >> $GITHUB_OUTPUT

  ##############################
  # SBOM / SCAN JOB
  ##############################
  sbom_scan:
    needs: fetch_top_repos
    continue-on-error: true
    runs-on: ubuntu-latest

    strategy:
      fail-fast: false
      matrix:
        repo: ${{ fromJSON(needs.fetch_top_repos.outputs.repo_list) }}

    steps:

      - name: Install base deps (without docker.io)
        run: |
          sudo apt-get update -y
          # DO NOT install docker.io (causes containerd.io conflicts on Ubuntu 24.04)
          sudo apt-get install -y jq curl wget python3-pip gnupg lsb-release rpm nodejs npm
          pip install --upgrade pip
          pip install cyclonedx-bom pandas

      - name: Install Docker (official script — avoids containerd.io conflict)
        run: |
          set -euo pipefail
          # use official installer which avoids the containerd.io apt conflict
          curl -fsSL https://get.docker.com | sudo sh
          sudo usermod -aG docker $USER || true
          docker version || true

      - name: Install Trivy (with retry)
        run: |
          set -euo pipefail
          for i in 1 2 3; do
            curl -fsSL https://aquasecurity.github.io/trivy-repo/deb/public.key \
              | sudo gpg --dearmor -o /usr/share/keyrings/trivy.gpg \
              && echo "deb [signed-by=/usr/share/keyrings/trivy.gpg] https://aquasecurity.github.io/trivy-repo/deb $(lsb_release -cs) main" \
                | sudo tee /etc/apt/sources.list.d/trivy.list \
              && sudo apt-get update -y \
              && sudo apt-get install -y trivy && break || sleep 5
          done || true

      - name: Install Syft (with retry)
        run: |
          set -euo pipefail
          for i in 1 2 3; do
            mkdir -p $HOME/bin
            curl -sSfL https://raw.githubusercontent.com/anchore/syft/main/install.sh \
              | sh -s -- -b $HOME/bin && echo "$HOME/bin" >> $GITHUB_PATH && syft version && break || sleep 3
          done || true

      - name: Try install cdxgen (npm) and mark fallback
        run: |
          set -euo pipefail
          if npm --version >/dev/null 2>&1; then
            sudo npm install -g @cyclonedx/cdxgen || true
          fi
          if command -v cdxgen >/dev/null 2>&1; then
            rm -f /tmp/cdxgen_fallback || true
          else
            echo "docker" > /tmp/cdxgen_fallback
          fi

      - name: Verify Syft
        run: syft version || true

      - name: Checkout repository (no submodules)
        uses: actions/checkout@v4
        with:
          repository: ${{ matrix.repo }}
          token: ${{ github.token }}
          path: project-src
          fetch-depth: 1
          submodules: false

      ##############################
      # LICENSE DISCOVERY
      ##############################
      - name: GitHub license metadata
        env:
          REPO: ${{ matrix.repo }}
          GH_TOKEN: ${{ github.token }}
        run: |
          gh api "/repos/$REPO/license" > project-src/gh-license.json 2>/dev/null \
            || echo '{"license":{"spdx_id":"NOASSERTION"}}' > project-src/gh-license.json

      - name: Local SPDX detection
        run: |
          FILE=$(find project-src -maxdepth=6 -type f \
            -iregex ".*(LICENSE|COPYING|COPYRIGHT|LICENSE.md|LICENSE.txt)" | head -1)
          if [ -n "$FILE" ]; then cp "$FILE" project-src/local-license.txt
          else echo "" > project-src/local-license.txt
          fi

      ##############################
      # SBOM GENERATION: Conan (optional), Trivy, Syft, Cdxgen
      ##############################

      - name: Conan SBOM (optional, retry)
        run: |
          set -euo pipefail
          cd project-src || exit 0
          for i in 1 2; do
            conan profile detect --force || true
            conan graph info . --format=json > conan-sbom.json && break || (echo '{"components":[]}' > conan-sbom.json && sleep 2)
          done || true

      - name: Trivy SBOM (CycloneDX) (retry)
        run: |
          set -euo pipefail
          for i in 1 2 3; do
            trivy fs --format cyclonedx -o project-src/trivy-fs.json project-src && break || (echo '{"components":[]}' > project-src/trivy-fs.json && sleep 3)
          done || true

      - name: Syft SBOM (retry)
        run: |
          set -euo pipefail
          for i in 1 2 3; do
            syft dir:project-src -o cyclonedx-json > project-src/syft-sbom.json && break || (echo '{"components":[]}' > project-src/syft-sbom.json && sleep 2)
          done || true

      - name: Cdxgen SBOM (local or docker fallback)
        run: |
          set -euo pipefail
          if [ -f /tmp/cdxgen_fallback ] && grep -q docker /tmp/cdxgen_fallback 2>/dev/null; then
            docker run --rm -v "$PWD"/project-src:/app:rw -t ghcr.io/cyclonedx/cdxgen:latest \
              cdxgen -o /app/cdxgen-sbom.json /app || echo '{"components":[]}' > project-src/cdxgen-sbom.json
          else
            if command -v cdxgen >/dev/null 2>&1; then
              cdxgen -o project-src/cdxgen-sbom.json project-src || echo '{"components":[]}' > project-src/cdxgen-sbom.json
            else
              echo '{"components":[]}' > project-src/cdxgen-sbom.json
            fi
          fi

      - name: Normalize SBOM arrays (safe)
        run: |
          for f in project-src/trivy-fs.json project-src/syft-sbom.json project-src/cdxgen-sbom.json project-src/conan-sbom.json; do
            if [ -f "$f" ]; then
              jq 'if has("components") then . else (. + {components: []}) end |
                 .components |= (if type=="array" then . else [] end)' "$f" > tmp && mv tmp "$f"
            else
              echo '{"components":[]}' > "$f"
            fi
          done

      ##############################
      # MERGE SBOMS (Trivy + Syft + Cdxgen + Conan) — preserve component licenses
      ##############################
      - name: Merge SBOMs
        run: |
          python3 <<'PY'
          import json, os
          def load(p):
              try: return json.load(open(p))
              except: return {"components":[]}
          trivy = load("project-src/trivy-fs.json")
          syft = load("project-src/syft-sbom.json")
          cdx = load("project-src/cdxgen-sbom.json")
          conan = load("project-src/conan-sbom.json")
          tcom = trivy.get("components",[]) or []
          scom = syft.get("components",[]) or []
          ccom = cdx.get("components",[]) or []
          gcom = conan.get("components",[]) or []
          # build enrichment indexes from syft + cdx
          idx_purl = {}
          idx_nv = {}
          for comp in (scom + ccom):
              if comp.get("purl"):
                  idx_purl[comp.get("purl")] = comp
              key=(comp.get("name"), comp.get("version"))
              if key not in idx_nv:
                  idx_nv[key] = comp
          def enrich(a,b):
              for k in ["supplier","purl","hashes","externalReferences","evidence","type","properties","copyright","licenses"]:
                  if not a.get(k) and b.get(k):
                      a[k] = b.get(k)
              return a
          merged=[]
          seen=set()
          for c in tcom:
              key=(c.get("name"), c.get("version"))
              match = idx_purl.get(c.get("purl")) or idx_nv.get(key)
              new = dict(c)
              if match:
                  new = enrich(new, match)
              merged.append(new)
              seen.add(key)
          for c in scom + ccom + gcom:
              key=(c.get("name"), c.get("version"))
              if key not in seen:
                  merged.append(c)
                  seen.add(key)
          repo_license = ""
          if os.path.exists("project-src/final_license.txt"):
              repo_license = open("project-src/final_license.txt").read().strip()
          out = {"repo_license": repo_license, "components": merged}
          json.dump(out, open("project-src/merged-sbom.json","w"), indent=2)
          PY

      ##############################
      # FINAL REPO LICENSE (do not overwrite component licenses)
      ##############################
      - name: Determine final repository license
        run: |
          GH=$(jq -r '.license.spdx_id // empty' project-src/gh-license.json)
          FILE=$(grep -Eo "SPDX-License-Identifier:\s*[A-Za-z0-9.\-+]+" project-src/local-license.txt | head -1 | awk -F': ' '{print $2}')
          FINAL="${GH:-$FILE}"
          [ -z "$FINAL" ] && FINAL="NOASSERTION"
          echo "$FINAL" > project-src/final_license.txt

      - name: Ensure merged-sbom.json contains repo_license (inject if missing)
        run: |
          L=$(cat project-src/final_license.txt)
          jq --arg lic "$L" '.repo_license = $lic' project-src/merged-sbom.json > tmp && mv tmp project-src/merged-sbom.json

      ##############################
      # TRIVY VULNERABILITIES (FULL)
      ##############################
      - name: Trivy filesystem vulnerability scan (full)
        run: |
          set -euo pipefail
          for i in 1 2; do
            trivy fs --format json -o project-src/vulnerabilities_fs.json project-src && break || (echo '{"Results":[]}' > project-src/vulnerabilities_fs.json && sleep 3)
          done || true

      - name: Trivy SBOM vulnerability scan (full)
        run: |
          set -euo pipefail
          for i in 1 2; do
            trivy sbom --format json -o project-src/vulnerabilities_sbom.json project-src/merged-sbom.json && break || (echo '{"Results":[]}' > project-src/vulnerabilities_sbom.json && sleep 3)
          done || true

      ##############################
      # Create unified vulnerabilities list (json + csv) in project-src/
      ##############################
      - name: Create unified vulnerabilities list (json + csv)
        run: |
          python3 <<'PY'
          import json, csv
          def extract(path, source):
              try:
                  data=json.load(open(path))
              except:
                  return []
              vulns=[]
              for r in data.get("Results",[]):
                  for v in r.get("Vulnerabilities",[]) or []:
                      vulns.append({
                          "id": v.get("VulnerabilityID"),
                          "package": v.get("PkgName"),
                          "version": v.get("InstalledVersion"),
                          "severity": v.get("Severity"),
                          "cvss": v.get("CVSS",{}),
                          "description": (v.get("Description") or "").strip(),
                          "source": source
                      })
              return vulns
          fs = extract("project-src/vulnerabilities_fs.json","filesystem")
          sb = extract("project-src/vulnerabilities_sbom.json","sbom")
          allv = fs + sb
          json.dump(allv, open("project-src/vulnerabilities.json","w"), indent=2)
          with open("project-src/vulnerabilities.csv","w", newline='') as f:
              w = csv.writer(f)
              w.writerow(["id","package","version","severity","source","description"])
              for v in allv:
                  w.writerow([v.get("id"), v.get("package"), v.get("version"), v.get("severity"), v.get("source"), v.get("description")])
          PY

      ##############################
      # Create libraries list (libraries.json + csv) in project-src/
      ##############################
      - name: Create libraries list (json + csv)
        run: |
          python3 <<'PY'
          import json, csv
          try:
              data=json.load(open("project-src/merged-sbom.json"))
          except:
              data={"components":[]}
          libs=[]
          for c in data.get("components",[]):
              supplier = None
              supp = c.get("supplier")
              if isinstance(supp, dict):
                  supplier = supp.get("name")
              elif supp:
                  supplier = supp
              libs.append({
                  "name": c.get("name"),
                  "version": c.get("version"),
                  "purl": c.get("purl"),
                  "supplier": supplier
              })
          json.dump(libs, open("project-src/libraries.json","w"), indent=2)
          with open("project-src/libraries.csv","w", newline='') as f:
              w=csv.writer(f)
              w.writerow(["name","version","purl","supplier"])
              for l in libs:
                  w.writerow([l.get("name"), l.get("version"), l.get("purl"), l.get("supplier")])
          PY

      ##############################
      # NTIA + BSI V2 + SCORE (write JSON outputs)
      ##############################
      - name: NTIA compliance (json)
        run: |
          docker run --rm -v $PWD/project-src:/sbom \
            ghcr.io/interlynk-io/sbomqs \
              compliance --ntia --format json /sbom/merged-sbom.json \
            > project-src/compliance_NTIA.json \
            || echo '{"error":true}' > project-src/compliance_NTIA.json
      - name: BSI-V2 compliance (json)
        run: |
          docker run --rm -v $PWD/project-src:/sbom \
            ghcr.io/interlynk-io/sbomqs \
              compliance --bsi-v2 --format json /sbom/merged-sbom.json \
            > project-src/compliance_BSI.json \
            || echo '{"error":true}' > project-src/compliance_BSI.json
      - name: SBOM Score (json)
        run: |
          docker run --rm -v $PWD/project-src:/sbom \
            ghcr.io/interlynk-io/sbomqs \
              score /sbom/merged-sbom.json --json \
            > project-src/score.json \
            || echo '{"error":true}' > project-src/score.json

      ##############################
      # Sanitize artifact name and upload all project-src files
      ##############################
      - name: Sanitize artifact name
        id: sanitize
        run: |
          SAFE="${{ matrix.repo }}"
          SAFE="${SAFE//\//_}"
          echo "name=$SAFE" >> $GITHUB_OUTPUT

      - name: Upload results (per-repo artifact)
        uses: actions/upload-artifact@v4
        with:
          name: sbom-reports-${{ steps.sanitize.outputs.name }}
          path: project-src/
          if-no-files-found: warn

  ##############################
  # AGGREGATION (collect per-repo artifacts and build summary.csv)
  ##############################
  aggregate_results:
    needs: sbom_scan
    runs-on: ubuntu-latest
    steps:

      - name: Download artifacts
        uses: actions/download-artifact@v4
        with:
          path: results/
          merge-multiple: true

      - name: Install Python deps
        run: |
          sudo apt-get update -y
          sudo apt-get install -y python3-pip
          pip install pandas

      - name: Build bulletproof summary.csv (recursive, normalizes nested artifacts)
        run: |
          python3 <<'PY'
          import os, glob, json, hashlib, pandas as pd, datetime, re

          def load(path):
              try:
                  return json.load(open(path))
              except:
                  return None

          def sha256_of_file(path):
              try:
                  import hashlib
                  h = hashlib.sha256()
                  with open(path, "rb") as f:
                      for chunk in iter(lambda: f.read(65536), b""):
                          h.update(chunk)
                  return h.hexdigest()
              except Exception:
                  return "error:no_file"

          rows = []

          # Find all merged-sbom.json files anywhere under results/
          candidates = glob.glob("results/**/merged-sbom.json", recursive=True)

          # Also consider any file ending with merged-sbom.json or *merged-sbom.json
          candidates += [p for p in glob.glob("results/**", recursive=True) if p.endswith("merged-sbom.json") and p not in candidates]

          # Deduplicate
          candidates = sorted(set(candidates))

          # If none found, try to find any cyclonedx-like files (trivy/syft/cdxgen)
          if not candidates:
              alt = glob.glob("results/**/trivy-fs.json", recursive=True) + \
                    glob.glob("results/**/syft-sbom.json", recursive=True) + \
                    glob.glob("results/**/cdxgen-sbom.json", recursive=True)
              candidates = sorted(set(alt))

          # For each candidate, compute its artifact root (the artifact folder)
          seen_roots = set()
          for merged_path in candidates:
              # Find nearest ancestor that looks like an artifact folder
              # e.g., results/sbom-reports-.../merged-sbom.json  OR
              #       results/sbom-reports-.../project-src/merged-sbom.json OR nested deeper
              merged_abs = os.path.abspath(merged_path)
              # walk up until we find a directory whose name starts with sbom-reports- or until results/
              cur = os.path.dirname(merged_abs)
              root_candidate = None
              while True:
                  if os.path.basename(cur).startswith("sbom-reports-") or os.path.basename(cur) == "results":
                      root_candidate = cur
                      break
                  parent = os.path.dirname(cur)
                  if parent == cur:
                      root_candidate = cur
                      break
                  cur = parent
              # choose artifact_dir as the folder under results/ that contains the artifact name
              # ensure it's under results/
              if "results" in root_candidate:
                  # prefer the sbom-reports-* folder
                  parts = root_candidate.split(os.sep)
                  try:
                      idx = parts.index("results")
                      # artifact_dir is results/<next-part> (if present)
                      if idx + 1 < len(parts):
                          artifact_dir = os.path.join(*parts[:idx+2])
                      else:
                          artifact_dir = root_candidate
                  except ValueError:
                      artifact_dir = root_candidate
              else:
                  artifact_dir = os.path.dirname(merged_abs)

              artifact_dir = os.path.normpath(artifact_dir) + os.sep

              if artifact_dir in seen_roots:
                  continue
              seen_roots.add(artifact_dir)

              # Now search within artifact_dir for the actual files (handle nested subfolder)
              # prefer locating files nearest merged_path
              base_paths = [artifact_dir,
                            os.path.join(artifact_dir, "project-src") + os.sep,
                            os.path.join(artifact_dir, os.path.basename(artifact_dir.rstrip(os.sep))) + os.sep]

              # Also include any directories under artifact_dir (first level)
              try:
                  subdirs = [os.path.join(artifact_dir, d) + os.sep for d in os.listdir(artifact_dir) if os.path.isdir(os.path.join(artifact_dir, d))]
                  base_paths = base_paths + subdirs
              except Exception:
                  pass

              merged = None
              for b in base_paths:
                  candidate = os.path.join(b, "merged-sbom.json")
                  if os.path.exists(candidate):
                      merged = load(candidate)
                      base_path = b
                      break
              # fallback: if merged_path exists use its dir
              if merged is None and os.path.exists(merged_path):
                  merged = load(merged_path)
                  base_path = os.path.dirname(merged_path) + os.sep
              if merged is None:
                  # try any merged file somewhere under artifact_dir
                  for p in glob.glob(os.path.join(artifact_dir, "**/*merged-sbom.json"), recursive=True):
                      merged = load(p)
                      base_path = os.path.dirname(p) + os.sep
                      break
              if merged is None:
                  # nothing to aggregate here
                  continue

              # load supporting files if present
              def try_load(fname):
                  for b in base_paths:
                      p = os.path.join(b, fname)
                      if os.path.exists(p):
                          return load(p) or {}
                  # also try anywhere under artifact_dir
                  for p in glob.glob(os.path.join(artifact_dir, "**", fname), recursive=True):
                      try:
                          return load(p) or {}
                      except:
                          pass
                  return {}

              trivy = try_load("trivy-fs.json") or {}
              syft = try_load("syft-sbom.json") or {}
              cdx = try_load("cdxgen-sbom.json") or {}
              conan = try_load("conan-sbom.json") or {}
              bsi = try_load("compliance_BSI.json")
              ntia = try_load("compliance_NTIA.json")
              score = try_load("score.json")
              gh = try_load("gh-license.json") or {}
              gh_spdx = gh.get("license",{}).get("spdx_id","")
              gh_name = gh.get("license",{}).get("name","")
              # local license
              local_txt = ""
              for b in base_paths:
                  p = os.path.join(b, "local-license.txt")
                  if os.path.exists(p):
                      try:
                          local_txt = open(p).read()
                          break
                      except:
                          local_txt = ""
              m = re.search(r"SPDX-License-Identifier:\s*([A-Za-z0-9.\-+]+)", local_txt or "")
              local_spdx = m.group(1) if m else ""
              final_license = ""
              for b in base_paths:
                  p = os.path.join(b, "final_license.txt")
                  if os.path.exists(p):
                      try:
                          final_license = open(p).read().strip()
                          break
                      except:
                          final_license = ""
              # components
              comps = merged.get("components",[]) or []
              names=[c.get("name") for c in comps]
              vers=[c.get("version") for c in comps]
              purls=[c.get("purl") for c in comps]
              suppliers=[
                  (c.get("supplier",{}) if isinstance(c.get("supplier"),dict) else {"name":c.get("supplier")}).get("name")
                  for c in comps
              ]
              # compute sha256 for compliance files if present
              ntia_file = None
              bsi_file = None
              for b in base_paths:
                  p_nt = os.path.join(b, "compliance_NTIA.json")
                  p_bsi = os.path.join(b, "compliance_BSI.json")
                  if os.path.exists(p_nt):
                      ntia_file = p_nt
                  if os.path.exists(p_bsi):
                      bsi_file = p_bsi
              ntia_hash = sha256_of_file(ntia_file) if ntia_file else "error:no_file"
              bsi_hash = sha256_of_file(bsi_file) if bsi_file else "error:no_file"

              # determine engine string by scanning for engine files anywhere under base
              engines = set()
              for fpath in glob.glob(os.path.join(artifact_dir, "**/*"), recursive=True):
                  fname = os.path.basename(fpath)
                  if fname in ("trivy-fs.json","syft-sbom.json","cdxgen-sbom.json","conan-sbom.json"):
                      try:
                          data = load(fpath) or {}
                          if data.get("components"):
                              if fname=="trivy-fs.json": engines.add("trivy")
                              if fname=="syft-sbom.json": engines.add("syft")
                              if fname=="cdxgen-sbom.json": engines.add("cdxgen")
                              if fname=="conan-sbom.json": engines.add("conan")
                      except:
                          pass

              engine_str = ",".join(sorted(engines)) if engines else "unknown"

              # repo name derived from artifact dir name
              # artifact_dir expected like .../results/sbom-reports-owner_repo/
              repo_basename = os.path.basename(os.path.normpath(artifact_dir.rstrip(os.sep)))
              repo = repo_basename.replace("sbom-reports-","").replace("_","/")
              timestamp = datetime.datetime.utcnow().replace(microsecond=0).isoformat() + "Z"

              base = dict(
                  repository=repo,
                  github_license_spdx=gh_spdx,
                  github_license_name=gh_name,
                  local_spdx_license=local_spdx,
                  final_assigned_license=final_license,
                  num_components=len(comps),
                  num_trivy=len(trivy.get("components",[]) or []),
                  num_syft=len(syft.get("components",[]) or []),
                  num_conan=len(conan.get("components",[]) or []),
                  component_names=names,
                  component_versions=vers,
                  component_purls=purls,
                  component_suppliers=suppliers,
                  engine=engine_str,
                  ntia_sha256=ntia_hash,
                  bsi_v2_sha256=bsi_hash,
                  timestamp=timestamp
              )

              # Add rows for score / ntia / bsi if present
              if score:
                  row = {**base, "type":"score"}
                  # flatten score json fields if needed
                  try:
                      row.update(score if isinstance(score, dict) else {})
                  except:
                      pass
                  rows.append(row)
              if ntia:
                  row = {**base, "type":"ntia"}
                  try:
                      row.update(ntia if isinstance(ntia, dict) else {})
                  except:
                      pass
                  rows.append(row)
              if bsi:
                  row = {**base, "type":"bsi"}
                  try:
                      row.update(bsi if isinstance(bsi, dict) else {})
                  except:
                      pass
                  rows.append(row)

          # write output
          outdir = "sbom-reports"
          os.makedirs(outdir, exist_ok=True)
          if not rows:
              df = pd.DataFrame(columns=["repository","github_license_spdx","github_license_name","local_spdx_license","final_assigned_license","num_components","num_trivy","num_syft","num_conan","component_names","component_versions","component_purls","component_suppliers","engine","ntia_sha256","bsi_v2_sha256","timestamp","type"])
          else:
              df = pd.json_normalize(rows)
              # ensure consistent column order
              cols = ["repository","engine","type","ntia_sha256","bsi_v2_sha256","timestamp"]
              # append remaining columns if present
              for c in df.columns:
                  if c not in cols:
                      cols.append(c)
              df = df.reindex(columns=cols, fill_value="")
          df.to_csv(os.path.join(outdir,"summary.csv"), index=False)
          print("Wrote", os.path.join(outdir,"summary.csv"))
          PY

      - name: Upload sbom-reports (summary + per-repo)
        uses: actions/upload-artifact@v4
        with:
          name: sbom-reports
          path: sbom-reports/
