name: IoT Full SBOM Pipeline

on:
  workflow_dispatch:
    inputs:
      language:
        description: "Language filter"
        required: true
        default: "C++"

permissions:
  contents: read
  security-events: write
  actions: read

jobs:

  ##############################
  # FETCH TOP REPOS
  ##############################
  fetch_top_repos:
    runs-on: ubuntu-latest
    outputs:
      repo_list: ${{ steps.fetch.outputs.repo_list }}

    steps:
      - name: Install jq
        run: sudo apt-get update -y && sudo apt-get install -y jq

      - name: Fetch repositories
        id: fetch
        env:
          LANG: ${{ github.event.inputs.language }}
          GH_TOKEN: ${{ github.token }}
        run: |
          set -euo pipefail
          # Use a robust query — change if you want a different filter
          # topic:iot is usually a better match than topic:IoT
          RAW_Q="topic:iot language:$LANG fork:false archived:false"
          # Fallback: if topic search returns nothing, expand the query (keeps behavior robust)
          REPOS=$(gh api -X GET /search/repositories \
              --raw-field q="$RAW_Q" \
              --raw-field sort=stars \
              --raw-field order=desc \
              --raw-field per_page=100 \
              --jq '.items[].full_name' || true)
          # If nothing found, try broad language-only query
          if [ -z "$REPOS" ]; then
            RAW_Q="language:$LANG fork:false archived:false"
            REPOS=$(gh api -X GET /search/repositories \
              --raw-field q="$RAW_Q" \
              --raw-field sort=stars \
              --raw-field order=desc \
              --raw-field per_page=100 \
              --jq '.items[].full_name' || true)
          fi
          JSON_ARRAY=$(printf "%s\n" "$REPOS" | jq -R -s -c 'split("\n")[:-1]')
          echo "repo_list=$JSON_ARRAY" >> $GITHUB_OUTPUT
  ##############################
  # SBOM / SCAN JOB
  ##############################
  sbom_scan:
    needs: fetch_top_repos
    continue-on-error: true
    runs-on: ubuntu-latest

    strategy:
      fail-fast: false
      matrix:
        repo: ${{ fromJSON(needs.fetch_top_repos.outputs.repo_list) }}

    steps:

      ##############################
      # BASE INSTALL
      ##############################
      - name: Install base deps
        run: |
          sudo apt-get update -y
          sudo apt-get install -y jq curl wget python3-pip gnupg lsb-release rpm nodejs npm docker.io
          pip install --upgrade pip
          pip install cyclonedx-bom pandas

      ##############################
      # TRIVY INSTALL
      ##############################
      - name: Install Trivy
        run: |
          curl -fsSL https://aquasecurity.github.io/trivy-repo/deb/public.key \
            | sudo gpg --dearmor -o /usr/share/keyrings/trivy.gpg
          echo "deb [signed-by=/usr/share/keyrings/trivy.gpg] https://aquasecurity.github.io/trivy-repo/deb $(lsb_release -cs) main" \
            | sudo tee /etc/apt/sources.list.d/trivy.list
          sudo apt-get update -y && sudo apt-get install -y trivy

      ##############################
      # SYFT INSTALL
      ##############################
      - name: Install Syft
        run: |
          mkdir -p $HOME/bin
          curl -sSfL https://raw.githubusercontent.com/anchore/syft/main/install.sh \
            | sh -s -- -b $HOME/bin
          echo "$HOME/bin" >> $GITHUB_PATH

      ##############################
      # CDXGEN INSTALL (npm or docker fallback)
      ##############################
      - name: Install cdxgen or mark fallback
        run: |
          if npm --version >/dev/null 2>&1; then
            sudo npm install -g @cyclonedx/cdxgen || true
          fi
          if command -v cdxgen >/dev/null 2>&1; then
            rm -f /tmp/cdxgen_fallback || true
          else
            echo docker > /tmp/cdxgen_fallback
          fi

      - name: Verify Syft
        run: syft version || true

      ##############################
      # CHECKOUT TARGET REPO
      ##############################
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          repository: ${{ matrix.repo }}
          token: ${{ github.token }}
          path: project-src
          fetch-depth: 1

      ##############################
      # LICENSE DISCOVERY
      ##############################
      - name: GitHub license metadata
        env:
          REPO: ${{ matrix.repo }}
          GH_TOKEN: ${{ github.token }}
        run: |
          gh api "/repos/$REPO/license" \
            > project-src/gh-license.json 2>/dev/null \
            || echo '{"license":{"spdx_id":"NOASSERTION"}}' > project-src/gh-license.json

      - name: Local SPDX detection
        run: |
          FILE=$(find project-src -maxdepth=6 -type f \
            -iregex ".*(LICENSE|COPYING|COPYRIGHT|LICENSE.md|LICENSE.txt)" | head -1)
          if [ -n "$FILE" ]; then cp "$FILE" project-src/local-license.txt
          else echo "" > project-src/local-license.txt
          fi

      ##############################
      # SBOM GENERATION
      ##############################

      - name: Conan SBOM (optional)
        run: |
          cd project-src || exit 0
          conan profile detect --force || true
          conan graph info . --format=json > conan-sbom.json || echo '{"components":[]}' > conan-sbom.json

      - name: Trivy SBOM
        run: |
          trivy fs --format cyclonedx -o project-src/trivy-fs.json project-src \
            || echo '{"components":[]}' > project-src/trivy-fs.json

      - name: Syft SBOM
        run: |
          syft dir:project-src -o cyclonedx-json > project-src/syft-sbom.json \
            || echo '{"components":[]}' > project-src/syft-sbom.json

      - name: Cdxgen SBOM
        run: |
          if [ -f /tmp/cdxgen_fallback ]; then
            docker run --rm -v "$PWD"/project-src:/app:rw ghcr.io/cyclonedx/cdxgen:latest \
              cdxgen -o /app/cdxgen-sbom.json /app || echo '{"components":[]}' > project-src/cdxgen-sbom.json
          else
            cdxgen -o project-src/cdxgen-sbom.json project-src || echo '{"components":[]}' > project-src/cdxgen-sbom.json
          fi

      - name: Normalize SBOM arrays
        run: |
          for f in project-src/*.json; do
            if grep -q '"components"' "$f" 2>/dev/null; then
              jq 'if (.components|type=="array") then . else (.components=[]) end' "$f" > tmp && mv tmp "$f"
            fi
          done

      ##############################
      # MERGE SBOMS (license-preserving)
      ##############################
      - name: Merge SBOMs
        run: |
          python3 <<'PY'
          import json, os

          def load(p):
              try: return json.load(open(p))
              except: return {"components":[]}

          trivy = load("project-src/trivy-fs.json")
          syft = load("project-src/syft-sbom.json")
          cdx  = load("project-src/cdxgen-sbom.json")
          conan= load("project-src/conan-sbom.json")

          merged=[]; seen=set()

          # index enrichers
          index={}
          for comp in syft.get("components",[]) + cdx.get("components",[]):
              key=(comp.get("name"), comp.get("version"))
              index[key]=comp

          def enrich(a,b):
              for k in ["purl","supplier","licenses","hashes","properties","evidence","externalReferences"]:
                  if not a.get(k) and b.get(k):
                      a[k]=b[k]
              return a

          # merge trivy first
          for c in trivy.get("components",[]):
              key=(c.get("name"),c.get("version"))
              base=dict(c)
              if key in index:
                  base=enrich(base,index[key])
              merged.append(base)
              seen.add(key)

          # then syft + cdx + conan
          for c in syft.get("components",[]) + cdx.get("components",[]) + conan.get("components",[]):
              key=(c.get("name"),c.get("version"))
              if key not in seen:
                  merged.append(c)
                  seen.add(key)

          repo_license=""
          if os.path.exists("project-src/local-license.txt"):
              txt=open("project-src/local-license.txt").read().strip()
              if txt: repo_license=txt

          json.dump({"repo_license":repo_license,"components":merged},
                    open("project-src/merged-sbom.json","w"),indent=2)
          PY

      ##############################
      # VULNERABILITY SCANS
      ##############################
      - name: Trivy FS vulnerabilities
        run: |
          trivy fs --format json -o project-src/vulnerabilities_fs.json project-src \
            || echo '{"Results":[]}' > project-src/vulnerabilities_fs.json

      - name: Trivy SBOM vulnerabilities
        run: |
          trivy sbom --format json -o project-src/vulnerabilities_sbom.json project-src/merged-sbom.json \
            || echo '{"Results":[]}' > project-src/vulnerabilities_sbom.json

      ##############################
      # BUILD unified vulnerabilities
      ##############################
      - name: Build vulnerabilities.json + csv
        run: |
          python3 <<'PY'
          import json, csv

          def load(p):
              try: return json.load(open(p))
              except: return {"Results":[]}

          fs=load("project-src/vulnerabilities_fs.json")
          sb=load("project-src/vulnerabilities_sbom.json")

          out=[]

          def extract(data,src):
              for r in data.get("Results",[]):
                  for v in r.get("Vulnerabilities",[]) or []:
                      out.append({
                          "id":v.get("VulnerabilityID"),
                          "package":v.get("PkgName"),
                          "version":v.get("InstalledVersion"),
                          "severity":v.get("Severity"),
                          "source":src,
                          "description":(v.get("Description") or "").strip()
                      })

          extract(fs,"filesystem")
          extract(sb,"sbom")

          json.dump(out,open("project-src/vulnerabilities.json","w"),indent=2)

          with open("project-src/vulnerabilities.csv","w",newline="") as f:
              w=csv.writer(f)
              w.writerow(["id","package","version","severity","source","description"])
              for v in out:
                  w.writerow([v[k] for k in ["id","package","version","severity","source","description"]])
          PY

      ##############################
      # BUILD libraries.json + csv
      ##############################
      - name: Build libraries list
        run: |
          python3 <<'PY'
          import json, csv

          data=json.load(open("project-src/merged-sbom.json"))
          libs=[]

          for c in data.get("components",[]):
              supplier=None
              s=c.get("supplier")
              if isinstance(s,dict): supplier=s.get("name")
              elif isinstance(s,str): supplier=s

              libs.append({
                  "name":c.get("name"),
                  "version":c.get("version"),
                  "purl":c.get("purl"),
                  "supplier":supplier
              })

          json.dump(libs,open("project-src/libraries.json","w"),indent=2)

          with open("project-src/libraries.csv","w",newline="") as f:
              w=csv.writer(f)
              w.writerow(["name","version","purl","supplier"])
              for l in libs:
                  w.writerow([l["name"],l["version"],l["purl"],l["supplier"]])
          PY

      ##############################
      # NTIA, BSI-V2, SCORE
      ##############################
      - name: NTIA compliance
        run: |
          docker run --rm -v $PWD/project-src:/sbom ghcr.io/interlynk-io/sbomqs \
            compliance --ntia --format json /sbom/merged-sbom.json \
            > project-src/compliance_NTIA.json \
            || echo '{"error":true}' > project-src/compliance_NTIA.json

      - name: BSI-V2 compliance
        run: |
          docker run --rm -v $PWD/project-src:/sbom ghcr.io/interlynk-io/sbomqs \
            compliance --bsi-v2 --format json /sbom/merged-sbom.json \
            > project-src/compliance_BSI.json \
            || echo '{"error":true}' > project-src/compliance_BSI.json

      - name: SBOM Score
        run: |
          docker run --rm -v $PWD/project-src:/sbom ghcr.io/interlynk-io/sbomqs \
            score /sbom/merged-sbom.json --json \
            > project-src/score.json \
            || echo '{"error":true}' > project-src/score.json

      ##############################
      # UPLOAD PER-REPO ARTIFACT
      ##############################
      - name: Sanitize artifact name
        id: sanitize
        run: |
          SAFE="${{ matrix.repo }}"
          SAFE="${SAFE//\//_}"
          echo "name=$SAFE" >> $GITHUB_OUTPUT

      - name: Upload results
        uses: actions/upload-artifact@v4
        with:
          name: sbom-reports-${{ steps.sanitize.outputs.name }}
          path: project-src/
          if-no-files-found: warn
  ##############################
  # AGGREGATION (collect per-repo artifacts and build summary.csv)
  ##############################
  aggregate_results:
    needs: sbom_scan
    runs-on: ubuntu-latest

    steps:
      - name: Download artifacts
        uses: actions/download-artifact@v4
        with:
          path: results/
          merge-multiple: true

      - name: Install Python deps
        run: |
          sudo apt-get update -y
          sudo apt-get install -y python3-pip
          pip install pandas

      - name: Build summary.csv (repo_name, engine, ntia_sha256, bsi_v2_sha256, timestamp)
        run: |
          python3 <<'PY'
          import os, glob, json, hashlib, pandas as pd, datetime

          def load(path):
              try:
                  return json.load(open(path))
              except:
                  return None

          def sha256_of_file(path):
              if not os.path.exists(path):
                  return "error:no_file"
              h = hashlib.sha256()
              with open(path, "rb") as f:
                  for chunk in iter(lambda: f.read(65536), b""):
                      h.update(chunk)
              return h.hexdigest()

          rows = []

          # Each item inside "results/" is a repo artifact
          for artifact_dir in glob.glob("results/*/"):
              if not artifact_dir.endswith("/"):
                  artifact_dir += "/"

              # Possible root paths
              bases = [
                  artifact_dir,
                  os.path.join(artifact_dir, "project-src") + "/"
              ]

              merged = None
              base_path = None

              # 1) Try canonical path
              for b in bases:
                  p = os.path.join(b, "merged-sbom.json")
                  if os.path.exists(p):
                      merged = load(p)
                      base_path = b
                      break

              # 2) Try fallback pattern (*merged-sbom.json)
              if merged is None:
                  for f in glob.glob(os.path.join(artifact_dir, "*merged-sbom.json")):
                      merged = load(f)
                      base_path = os.path.dirname(f) + "/"
                      break

              if merged is None:
                  # No SBOM found — skip
                  continue

              # Compliance files
              ntia_file = os.path.join(base_path, "compliance_NTIA.json")
              bsi_file  = os.path.join(base_path, "compliance_BSI.json")

              ntia_hash = sha256_of_file(ntia_file)
              bsi_hash  = sha256_of_file(bsi_file)

              # Engine detection
              engines = []
              for eng_name, fname in [
                  ("trivy","trivy-fs.json"),
                  ("syft","syft-sbom.json"),
                  ("cdxgen","cdxgen-sbom.json"),
                  ("conan","conan-sbom.json")
              ]:
                  p = os.path.join(base_path, fname)
                  if os.path.exists(p):
                      data = load(p) or {}
                      if data.get("components"):
                          engines.append(eng_name)
              engine_str = ",".join(sorted(set(engines))) if engines else "unknown"

              # Repo name from artifact folder
              artifact_name = os.path.basename(os.path.normpath(artifact_dir.rstrip("/")))
              repo_name = artifact_name.replace("sbom-reports-","").replace("_","/")

              timestamp = datetime.datetime.utcnow().replace(microsecond=0).isoformat() + "Z"

              rows.append({
                  "repo_name": repo_name,
                  "engine": engine_str,
                  "ntia_sha256": ntia_hash,
                  "bsi_v2_sha256": bsi_hash,
                  "timestamp": timestamp
              })

          if not rows:
              df = pd.DataFrame(columns=[
                  "repo_name","engine","ntia_sha256","bsi_v2_sha256","timestamp"
              ])
          else:
              df = pd.DataFrame(rows)

          df.to_csv("summary.csv", index=False)
          PY

      - name: Upload summary.csv
        uses: actions/upload-artifact@v4
        with:
          name: sbom-summary
          path: summary.csv
