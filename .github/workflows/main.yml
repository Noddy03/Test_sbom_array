# Final fixed SBOM workflow (clean, robust, based on your core)
name: SBOM Matrix Scan (cdxgen-primary, trivy-analyzer, syft-enrich)

on:
  workflow_dispatch:
    inputs:
      language:
        description: "Programming language filter (e.g. C++, Python, Java)"
        required: true
        default: "C++"

jobs:
  fetch_top_repos:
    runs-on: ubuntu-latest
    outputs:
      repo_list: ${{ steps.fetch.outputs.repo_list }}
    steps:
      - name: Install gh & jq
        run: |
          sudo apt-get update -y
          sudo apt-get install -y jq curl git || true
          if ! command -v gh >/dev/null 2>&1; then
            curl -s https://raw.githubusercontent.com/cli/cli/trunk/script/install.sh | sudo bash || true
          fi

      - name: Fetch repositories (top 100)
        id: fetch
        env:
          LANG: ${{ github.event.inputs.language }}
        run: |
          set -euo pipefail
          RAW_Q="topic:iot language:$LANG fork:false archived:false"
          REPOS=$(gh api -X GET /search/repositories \
              --raw-field q="$RAW_Q" \
              --raw-field sort=stars \
              --raw-field order=desc \
              --raw-field per_page=100 \
              --jq '.items[].full_name' || true)
          if [ -z "$REPOS" ]; then
            RAW_Q="language:$LANG fork:false archived:false"
            REPOS=$(gh api -X GET /search/repositories \
              --raw-field q="$RAW_Q" \
              --raw-field sort=stars \
              --raw-field order=desc \
              --raw-field per_page=100 \
              --jq '.items[].full_name' || true)
          fi
          JSON_ARRAY=$(printf "%s\n" "$REPOS" | jq -R -s -c 'split("\n")[:-1]')
          echo "repo_list=$JSON_ARRAY" >> $GITHUB_OUTPUT

  sbom_scan:
    needs: fetch_top_repos
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        repo: ${{ fromJSON(needs.fetch_top_repos.outputs.repo_list) }}

    steps:
      - name: Disable git submodule recursion
        run: |
          git config --global submodule.recurse false

      - name: Install prerequisites
        run: |
          sudo apt-get update -y
          sudo apt-get install -y python3-pip jq git curl wget npm docker.io || true
          pip3 install --upgrade pip || true

      - name: Install Syft
        run: |
          mkdir -p $HOME/bin
          curl -sSfL https://raw.githubusercontent.com/anchore/syft/main/install.sh | sh -s -- -b $HOME/bin || true
          echo "$HOME/bin" >> $GITHUB_PATH || true

      - name: Install Trivy
        run: |
          curl -sfL https://raw.githubusercontent.com/aquasecurity/trivy/main/contrib/install.sh | sudo sh -s -- -b /usr/local/bin || true

      - name: Install cdxgen (or fallback to docker)
        run: |
          if npm --version >/dev/null 2>&1; then
            sudo npm install -g @cyclonedx/cdxgen || true
          fi
          if ! command -v cdxgen >/dev/null 2>&1; then
            echo "docker" > /tmp/cdxgen_fallback
          fi

      - name: Checkout target repository (shallow, no submodules)
        uses: actions/checkout@v4
        with:
          repository: ${{ matrix.repo }}
          path: project-src
          submodules: false
          fetch-depth: 1
          token: ${{ secrets.GH_PAT || github.token }}

      - name: List repository top
        run: |
          echo "Repo: ${{ matrix.repo }}"
          ls -la project-src | sed -n '1,200p' || true

      - name: Collect GitHub license metadata
        env:
          REPO: ${{ matrix.repo }}
        run: |
          gh api "/repos/$REPO/license" > project-src/gh-license.json 2>/dev/null || echo '{"license":{"spdx_id":"NOASSERTION"}}' > project-src/gh-license.json

      - name: Detect local license file
        run: |
          FILE=$(find project-src -maxdepth 6 -type f -iregex ".*LICENSE.*" | head -1)
          if [ -n "$FILE" ]; then cp "$FILE" project-src/local-license.txt; else echo "" > project-src/local-license.txt; fi

      - name: Generate SBOM with cdxgen (primary)
        run: |
          if [ -f /tmp/cdxgen_fallback ]; then
            docker run --rm -v "$PWD/project-src:/app" ghcr.io/cyclonedx/cdxgen:latest cdxgen -o /app/cdxgen-sbom.json /app || echo '{"components":[]}' > project-src/cdxgen-sbom.json
          else
            cdxgen -o project-src/cdxgen-sbom.json project-src || echo '{"components":[]}' > project-src/cdxgen-sbom.json
          fi

      - name: Syft SBOM (enrich)
        run: |
          syft dir:project-src -o cyclonedx-json > project-src/syft-sbom.json || echo '{"components":[]}' > project-src/syft-sbom.json

      - name: Trivy SBOM (fallback/enhancer)
        run: |
          trivy fs --format cyclonedx -o project-src/trivy-fs.json project-src || echo '{"components":[]}' > project-src/trivy-fs.json

      - name: Conan optional
        run: |
          cd project-src || exit 0
          if ls conanfile.* >/dev/null 2>&1; then
            conan profile detect --force || true
            conan graph info . --format=json > conan-sbom.json || echo '{"components":[]}' > conan-sbom.json
          else
            echo '{"components":[]}' > conan-sbom.json
          fi

      - name: Normalize SBOMs
        run: |
          for f in project-src/trivy-fs.json project-src/syft-sbom.json project-src/cdxgen-sbom.json project-src/conan-sbom.json; do
            if [ -f "$f" ]; then
              jq 'if has("components") then . else (. + {components: []}) end | .components |= (if type=="array" then . else [] end)' "$f" > tmp && mv tmp "$f"
            else
              echo '{"components":[]}' > "$f"
            fi
          done

      - name: Merge SBOMs (cdxgen primary, syft enrich, trivy enhance, conan optional)
        run: |
          python3 <<'PY'
          import json, os, copy

          def load(p):
              try:
                  return json.load(open(p))
              except:
                  return {"components": []}

          cdx = load("project-src/cdxgen-sbom.json")
          syf = load("project-src/syft-sbom.json")
          tri = load("project-src/trivy-fs.json")
          con = load("project-src/conan-sbom.json")

          ccom = cdx.get("components", [])
          scom = syf.get("components", [])
          tcom = tri.get("components", [])
          gcom = con.get("components", [])

          def key_nv(c):
              return (c.get("name"), c.get("version"))

          def deep_merge(base, src):
              for k in [
                  "supplier", "purl", "type", "copyright",
                  "evidence", "properties", "externalReferences",
                  "hashes", "licenses"
              ]:
                  b = base.get(k)
                  s = src.get(k)
                  if not b and s:
                      base[k] = copy.deepcopy(s)
                  elif isinstance(b, list) and isinstance(s, list):
                      seen = {json.dumps(x, sort_keys=True) for x in b}
                      for item in s:
                          if json.dumps(item, sort_keys=True) not in seen:
                              b.append(item)
              for k, v in src.items():
                  if k not in base or base[k] in (None, ""):
                      base[k] = copy.deepcopy(v)
              return base

          merged = {}

          for c in ccom:
              p = c.get("purl")
              merged[p or key_nv(c)] = copy.deepcopy(c)

          for c in scom:
              p = c.get("purl")
              k = p if p in merged else key_nv(c)
              if k in merged:
                  merged[k] = deep_merge(merged[k], c)
              else:
                  merged[k] = copy.deepcopy(c)

          for c in tcom:
              p = c.get("purl")
              k = p if p in merged else key_nv(c)
              if k in merged:
                  merged[k] = deep_merge(merged[k], c)
              else:
                  if c.get("name"):
                      merged[k] = copy.deepcopy(c)

          for c in gcom:
              p = c.get("purl")
              k = p or key_nv(c)
              if k not in merged:
                  merged[k] = copy.deepcopy(c)

          final_components = list(merged.values())

          repo_license = ""
          lic_path = "project-src/local-license.txt"
          if os.path.exists(lic_path):
              txt = open(lic_path).read().strip()
              if "SPDX" in txt:
                  repo_license = txt

          out = {"repo_license": repo_license, "components": final_components}
          json.dump(out, open("project-src/merged-sbom.json", "w"), indent=2)
          PY

      - name: Ensure merged-sbom contains license
        run: |
          GH=$(jq -r '.license.spdx_id // empty' project-src/gh-license.json)
          FILE=$(grep -Eo "SPDX-License-Identifier:\\s*[A-Za-z0-9.\\-+]+" project-src/local-license.txt | head -1 | awk -F': ' '{print $2}')
          FINAL="${GH:-$FILE}"
          [ -z "$FINAL" ] && FINAL="NOASSERTION"
          echo "$FINAL" > project-src/final_license.txt
          jq --arg lic "$FINAL" '.repo_license = $lic' project-src/merged-sbom.json > tmp && mv tmp project-src/merged-sbom.json || true

      - name: Trivy SBOM vuln scan
        run: |
          trivy sbom --format json --vuln-type library,os -o project-src/vulnerabilities_sbom.json project-src/merged-sbom.json || echo '{"Results":[]}' > project-src/vulnerabilities_sbom.json

      - name: Trivy FS vuln scan
        run: |
          trivy fs --format json --vuln-type library,os -o project-src/vulnerabilities_fs.json project-src || echo '{"Results":[]}' > project-src/vulnerabilities_fs.json

      - name: Create unified vulnerabilities list
        run: |
          python3 <<'PY'
          import json, csv
          def extract(path, source):
              try:
                  data=json.load(open(path))
              except:
                  return []
              vulns=[]
              for r in data.get('Results',[]):
                  for v in r.get('Vulnerabilities',[]) or []:
                      vulns.append({
                          'id': v.get('VulnerabilityID'),
                          'package': v.get('PkgName'),
                          'version': v.get('InstalledVersion'),
                          'severity': v.get('Severity'),
                          'cvss': v.get('CVSS',{}),
                          'description': (v.get('Description') or '').strip(),
                          'source': source
                      })
              return vulns
          fs = extract('project-src/vulnerabilities_fs.json','filesystem')
          sb = extract('project-src/vulnerabilities_sbom.json','sbom')
          allv = fs + sb
          json.dump(allv, open('project-src/vulnerabilities.json','w'), indent=2)
          with open('project-src/vulnerabilities.csv','w', newline='') as f:
              w = csv.writer(f)
              w.writerow(['id','package','version','severity','source','description'])
              for v in allv:
                  w.writerow([v.get('id'), v.get('package'), v.get('version'), v.get('severity'), v.get('source'), v.get('description')])
          PY

      - name: Create libraries list
        run: |
          python3 <<'PY'
          import json, csv
          try:
              data=json.load(open('project-src/merged-sbom.json'))
          except:
              data={'components':[]}
          libs=[]
          for c in data.get('components',[]):
              supplier=None
              supp=c.get('supplier')
              if isinstance(supp, dict):
                  supplier=supp.get('name')
              elif supp:
                  supplier=supp
              libs.append({'name':c.get('name'),'version':c.get('version'),'purl':c.get('purl'),'supplier':supplier})
          json.dump(libs, open('project-src/libraries.json','w'), indent=2)
          with open('project-src/libraries.csv','w', newline='') as f:
              w=csv.writer(f)
              w.writerow(['name','version','purl','supplier'])
              for l in libs:
                  w.writerow([l.get('name'), l.get('version'), l.get('purl'), l.get('supplier')])
          PY

      - name: NTIA compliance
        run: |
          set -euo pipefail
          (sudo docker run --rm -v "$PWD/project-src:/sbom" ghcr.io/interlynk-io/sbomqs compliance --ntia --format json /sbom/merged-sbom.json > project-src/compliance_NTIA.json) || (echo '{"error":true}' > project-src/compliance_NTIA.json)

      - name: BSI compliance
        run: |
          set -euo pipefail
          (sudo docker run --rm -v "$PWD/project-src:/sbom" ghcr.io/interlynk-io/sbomqs compliance --bsi-v2 --format json /sbom/merged-sbom.json > project-src/compliance_BSI.json) || (echo '{"error":true}' > project-src/compliance_BSI.json)

      - name: SBOM score
        run: |
          set -euo pipefail
          (sudo docker run --rm -v "$PWD/project-src:/sbom" ghcr.io/interlynk-io/sbomqs score /sbom/merged-sbom.json --json > project-src/score.json) || (echo '{"error":true}' > project-src/score.json)

      - name: Prepare artifact name
        id: prep
        run: |
          SAFE="${{ matrix.repo }}"
          SAFE="${SAFE//\//_}"
          echo "name=$SAFE" >> $GITHUB_OUTPUT

      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: sbom-${{ steps.prep.outputs.name }}
          path: project-src/**
          if-no-files-found: warn

  aggregate_results:
    needs: sbom_scan
    runs-on: ubuntu-latest
    steps:
      - name: Download artifacts
        uses: actions/download-artifact@v4
        with:
          path: results/
          merge-multiple: false

      - name: Install python deps
        run: |
          sudo apt-get update -y
          sudo apt-get install -y python3-pip jq
          pip3 install pandas || true

      - name: Build global summary.csv
        run: |
          python3 <<'PY'
          import os, glob, json, hashlib, pandas as pd, datetime, re
          def load(path):
              try:
                  return json.load(open(path))
              except:
                  return None
          def sha256_of_file(path):
              try:
                  import hashlib
                  h = hashlib.sha256()
                  with open(path, "rb") as f:
                      for chunk in iter(lambda: f.read(65536), b""):
                          h.update(chunk)
                  return h.hexdigest()
              except Exception:
                  return "error:no_file"

          rows = []
          candidates = glob.glob("results/**/merged-sbom.json", recursive=True)
          candidates = sorted(set(candidates))
          if not candidates:
              alt = glob.glob("results/**/trivy-fs.json", recursive=True) + glob.glob("results/**/syft-sbom.json", recursive=True) + glob.glob("results/**/cdxgen-sbom.json", recursive=True)
              candidates = sorted(set(alt))

          seen_roots = set()
          for merged_path in candidates:
              merged_abs = os.path.abspath(merged_path)
              cur = os.path.dirname(merged_abs)
              root_candidate = None
              while True:
                  base = os.path.basename(cur)
                  if base.startswith("sbom-") or base == "results":
                      root_candidate = cur
                      break
                  parent = os.path.dirname(cur)
                  if parent == cur:
                      root_candidate = cur
                      break
                  cur = parent
              artifact_dir = os.path.normpath(root_candidate) + os.sep
              if artifact_dir in seen_roots:
                  continue
              seen_roots.add(artifact_dir)

              base_paths = [artifact_dir, os.path.join(artifact_dir, "project-src") + os.sep, os.path.join(artifact_dir, os.path.basename(artifact_dir.rstrip(os.sep))) + os.sep]
              try:
                  subdirs = [os.path.join(artifact_dir, d) + os.sep for d in os.listdir(artifact_dir) if os.path.isdir(os.path.join(artifact_dir, d))]
                  base_paths = base_paths + subdirs
              except Exception:
                  pass

              def try_load(fname):
                  for b in base_paths:
                      p = os.path.join(b, fname)
                      if os.path.exists(p):
                          return load(p) or {}
                  for p in glob.glob(os.path.join(artifact_dir, "**", fname), recursive=True):
                      try:
                          return load(p) or {}
                      except:
                          pass
                  return {}

              merged = None
              base_path = None
              for b in base_paths:
                  p = os.path.join(b, "merged-sbom.json")
                  if os.path.exists(p):
                      merged = load(p)
                      base_path = b
                      break
              if merged is None:
                  for p in glob.glob(os.path.join(artifact_dir, "**/*merged-sbom.json"), recursive=True):
                      merged = load(p)
                      base_path = os.path.dirname(p) + os.sep
                      break
              if merged is None:
                  continue

              trivy = try_load("trivy-fs.json") or {}
              syft = try_load("syft-sbom.json") or {}
              cdx = try_load("cdxgen-sbom.json") or {}
              conan = try_load("conan-sbom.json") or {}
              bsi = try_load("compliance_BSI.json")
              ntia = try_load("compliance_NTIA.json")
              score = try_load("score.json")
              gh = try_load("gh-license.json") or {}
              gh_spdx = gh.get("license",{}).get("spdx_id","")
              gh_name = gh.get("license",{}).get("name","")
              vuln_count = 0
              vuln_list = try_load("vulnerabilities.json") or {}
              if isinstance(vuln_list, list):
                  vuln_count = len(vuln_list)
              else:
                  for b in base_paths:
                      p = os.path.join(b, "vulnerabilities.csv")
                      if os.path.exists(p):
                          try:
                              with open(p, "r") as fh:
                                  vuln_count = sum(1 for _ in fh) - 1
                          except:
                              vuln_count = 0
                          break

              local_txt = ""
              for b in base_paths:
                  p = os.path.join(b, "local-license.txt")
                  if os.path.exists(p):
                      try:
                          local_txt = open(p).read()
                          break
                      except:
                          local_txt = ""
              m = re.search(r"SPDX-License-Identifier:\s*([A-Za-z0-9.+-]+)", local_txt or "")
              local_spdx = m.group(1) if m else ""

              final_license = ""
              for b in base_paths:
                  p = os.path.join(b, "final_license.txt")
                  if os.path.exists(p):
                      try:
                          final_license = open(p).read().strip()
                          break
                      except:
                          final_license = ""

              comps = merged.get("components",[]) or []
              names=[c.get("name") for c in comps]
              vers=[c.get("version") for c in comps]
              purls=[c.get("purl") for c in comps]
              suppliers=[
                  (c.get("supplier",{}) if isinstance(c.get("supplier"),dict) else {"name":c.get("supplier")}).get("name")
                  for c in comps
              ]

              ntia_file = None
              bsi_file = None
              for b in base_paths:
                  p_nt = os.path.join(b, "compliance_NTIA.json")
                  p_bsi = os.path.join(b, "compliance_BSI.json")
                  if os.path.exists(p_nt):
                      ntia_file = p_nt
                  if os.path.exists(p_bsi):
                      bsi_file = p_bsi
              ntia_hash = sha256_of_file(ntia_file) if ntia_file else "error:no_file"
              bsi_hash = sha256_of_file(bsi_file) if bsi_file else "error:no_file"

              engines = set()
              for fpath in glob.glob(os.path.join(artifact_dir, "**/*"), recursive=True):
                  fname = os.path.basename(fpath)
                  if fname in ("trivy-fs.json","syft-sbom.json","cdxgen-sbom.json","conan-sbom.json"):
                      try:
                          data = load(fpath) or {}
                          if data.get("components"):
                              if fname=="trivy-fs.json": engines.add("trivy")
                              if fname=="syft-sbom.json": engines.add("syft")
                              if fname=="cdxgen-sbom.json": engines.add("cdxgen")
                              if fname=="conan-sbom.json": engines.add("conan")
                      except:
                          pass
              engine_str = ",".join(sorted(engines)) if engines else "unknown"

              repo_basename = os.path.basename(os.path.normpath(artifact_dir.rstrip(os.sep)))
              repo = repo_basename.replace("sbom-","").replace("_","/") if repo_basename else ""
              timestamp = datetime.datetime.utcnow().replace(microsecond=0).isoformat() + "Z"

              base = dict(
                  repository=repo,
                  github_license_spdx=gh_spdx,
                  github_license_name=gh_name,
                  local_spdx_license=local_spdx,
                  final_assigned_license=final_license,
                  num_components=len(comps),
                  num_trivy=len(tri.get("components",[]) or []),
                  num_syft=len(syft.get("components",[]) or []),
                  num_conan=len(conan.get("components",[]) or []),
                  component_names=names,
                  component_versions=vers,
                  component_purls=purls,
                  component_suppliers=suppliers,
                  engine=engine_str,
                  ntia_sha256=ntia_hash,
                  bsi_v2_sha256=bsi_hash,
                  vuln_count=vuln_count,
                  timestamp=timestamp
              )

              def add_row(data, t):
                  row = {**base, "type": t}
                  if isinstance(data, dict):
                      row.update(data)
                  else:
                      row["error"] = True
                  rows.append(row)

              add_row(score or {}, "score")
              add_row(ntia or {}, "ntia")
              add_row(bsi or {}, "bsi")

          outdir = "sbom-reports"
          os.makedirs(outdir, exist_ok=True)
          if not rows:
              df = pd.DataFrame(columns=["repository","engine","type","ntia_sha256","bsi_v2_sha256","vuln_count","timestamp"])
          else:
              df = pd.json_normalize(rows)
              cols = ["repository","engine","type","ntia_sha256","bsi_v2_sha256","vuln_count","timestamp"]
              for c in df.columns:
                  if c not in cols:
                      cols.append(c)
              df = df.reindex(columns=cols, fill_value="")
          df.to_csv(os.path.join(outdir,"summary.csv"), index=False)
          print("Wrote", os.path.join(outdir,"summary.csv"))
          PY
